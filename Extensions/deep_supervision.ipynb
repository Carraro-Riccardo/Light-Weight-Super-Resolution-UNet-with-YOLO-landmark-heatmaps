{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVilGaX2n02x",
        "outputId": "c445976a-4bae-4aaa-faa1-86c68de21d3f"
      },
      "outputs": [],
      "source": [
        "!pip install -qq ultralytics\n",
        "!pip install -qq torchmetrics\n",
        "!pip install -qq lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhQkrGmuneWY",
        "outputId": "485937cf-d2bc-4ab4-f0aa-e2e9bdb91ace"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /kaggle/input/celeba-dataset\n",
            "Using device: cuda\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1116  100  1116    0     0   3009      0 --:--:-- --:--:-- --:--:--  3016\n",
            "100 1862M  100 1862M    0     0  52.9M      0  0:00:35  0:00:35 --:--:-- 54.5M\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "#dowload heatmaps\n",
        "#small 10k\n",
        "#!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps_10k.h5\n",
        "\n",
        "#medium 30k\n",
        "!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps_30k.h5\n",
        "\n",
        "#uncomment the following line to use the 50k version of the dataset\n",
        "#!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps.h5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HU9cNqQklC0y"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from os.path import join, splitext\n",
        "from PIL import Image\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class CelebDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    CelebA dataset with optional landmark-heatmap loading from HDF5.\n",
        "\n",
        "    Returns: (x2, x4, hr, lr, heatmap)\n",
        "      - x2: 32×32 target tensor\n",
        "      - x4: 64×64 target tensor\n",
        "      - hr: 128×128 target tensor\n",
        "      - lr: 16×16 input tensor\n",
        "      - heatmap: 1×128×128 float tensor\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str = './dataset/',\n",
        "        state: str = 'train',\n",
        "        data_augmentation: bool = False,\n",
        "        heatmap_h5: str = None,\n",
        "    ):\n",
        "        self.main_path = data_path\n",
        "        self.state = state\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.img_path = join(self.main_path, 'img_align_celeba/img_align_celeba/')\n",
        "        self.eval_partition_path = join(self.main_path, 'list_eval_partition.csv')\n",
        "\n",
        "        # load train/val/test split\n",
        "        train_list, val_list, test_list = [], [], []\n",
        "        with open(self.eval_partition_path, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for fname, split in reader:\n",
        "                fname, split = fname.strip(), split.strip()\n",
        "                if split == '0':\n",
        "                    train_list.append(fname)\n",
        "                elif split == '1':\n",
        "                    val_list.append(fname)\n",
        "                else:\n",
        "                    test_list.append(fname)\n",
        "\n",
        "        if state == 'train':\n",
        "            self.image_list = sorted(train_list)\n",
        "        elif state == 'val':\n",
        "            self.image_list = sorted(val_list)\n",
        "        else:\n",
        "            self.image_list = sorted(test_list)\n",
        "\n",
        "        # transforms\n",
        "        if state=='train' and data_augmentation:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "                transforms.RandomRotation(\n",
        "                    20,\n",
        "                    interpolation=InterpolationMode.BILINEAR\n",
        "                ),\n",
        "                transforms.ColorJitter(0.4,0.4,0.4,0.1)\n",
        "            ])\n",
        "        else:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "            ])\n",
        "\n",
        "        self.totensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "        ])\n",
        "        self.down64 = transforms.Resize((64,64))\n",
        "        self.down32 = transforms.Resize((32,32))\n",
        "        self.down16 = transforms.Resize((16,16))\n",
        "\n",
        "        # ACTUALLY load heatmaps into memory\n",
        "        if heatmap_h5:\n",
        "            with h5py.File(heatmap_h5, 'r') as h5_file:\n",
        "                # Load the entire heatmap dataset into RAM\n",
        "                self.heatmaps = np.array(h5_file['heatmaps'])  # Shape: (N, 128, 128)\n",
        "        else:\n",
        "            self.heatmaps = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image\n",
        "        fname = self.image_list[index]\n",
        "        img = Image.open(join(self.img_path, fname)).convert('RGB')\n",
        "        img = self.pre_process(img)\n",
        "\n",
        "        # build multi-scale\n",
        "        x4 = self.down64(img)    # 64x64\n",
        "        x2 = self.down32(x4)     # 32x32\n",
        "        lr = self.down16(x2)     # 16x16\n",
        "\n",
        "        # to tensor\n",
        "        hr_tensor = self.totensor(img)\n",
        "        x4_tensor = self.totensor(x4)\n",
        "        x2_tensor = self.totensor(x2)\n",
        "        lr_tensor = self.totensor(lr)\n",
        "\n",
        "        # load heatmap (already 128×128)\n",
        "        if self.heatmaps is not None:\n",
        "            hm = self.heatmaps[index]              # numpy array (128,128)\n",
        "            heat = torch.from_numpy(hm.copy()).unsqueeze(0)  # (1,128,128)\n",
        "            #print('heat')\n",
        "        else:\n",
        "            heat = torch.zeros(1,128,128)\n",
        "            #print('noheat')\n",
        "\n",
        "        return x2_tensor, x4_tensor, hr_tensor, lr_tensor, heat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "C0i-twj4vWzD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(in_ch, out_ch, stride=1):\n",
        "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            conv3x3(in_ch, out_ch, stride=stride),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            conv3x3(out_ch, out_ch),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_ch, skip_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv = DoubleConv(in_ch + skip_ch, out_ch)\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpLearn(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.up(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "        )\n",
        "    def forward(self, x): return x + self.body(x)\n",
        "\n",
        "class SuperResolutionUNet(nn.Module):\n",
        "    def __init__(self, in_channels=3, base_filters=32, out_channels=3, refine_blocks=3, deep_supervision=True):\n",
        "        super().__init__()\n",
        "        self.deep_supervision = deep_supervision\n",
        "\n",
        "        # Encoder (strided)\n",
        "        self.enc1 = DoubleConv(in_channels,       base_filters,   stride=1)  # 16x\n",
        "        self.enc2 = DoubleConv(base_filters,      base_filters*2, stride=2)  # 8x\n",
        "        self.enc3 = DoubleConv(base_filters*2,    base_filters*4, stride=2)  # 4x\n",
        "        self.enc4 = DoubleConv(base_filters*4,    base_filters*8, stride=2)  # 2x\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(base_filters*8, base_filters*8)\n",
        "\n",
        "        # Decoder\n",
        "        self.up3 = UpBlock(base_filters*8, base_filters*4, base_filters*4)  # 4x\n",
        "        self.up2 = UpBlock(base_filters*4, base_filters*2, base_filters*2)  # 8x\n",
        "        self.up1 = UpBlock(base_filters*2, base_filters,   base_filters)    # 16x\n",
        "\n",
        "        # Learned upsampling\n",
        "        self.up_learn1 = UpLearn(base_filters)  # 32x\n",
        "        self.up_learn2 = UpLearn(base_filters)  # 64x\n",
        "        self.up_learn3 = UpLearn(base_filters)  # 128x\n",
        "\n",
        "        # Refine head at 128x\n",
        "        self.refine_in = nn.Conv2d(base_filters, base_filters, 1)\n",
        "        self.refine = nn.Sequential(*[ResBlock(base_filters) for _ in range(refine_blocks)])\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, 1)\n",
        "\n",
        "        # --- Deep supervision heads ---\n",
        "        if self.deep_supervision:\n",
        "            self.out32 = nn.Conv2d(base_filters, out_channels, 1)  # Output for 32x32\n",
        "            self.out64 = nn.Conv2d(base_filters, out_channels, 1)  # Output for 64x64\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)        # 16x\n",
        "        e2 = self.enc2(e1)       # 8x\n",
        "        e3 = self.enc3(e2)       # 4x\n",
        "        e4 = self.enc4(e3)       # 2x\n",
        "\n",
        "        b  = self.bottleneck(e4)\n",
        "\n",
        "        # Decoder\n",
        "        d3 = self.up3(b, e3)     # 4x\n",
        "        d2 = self.up2(d3, e2)    # 8x\n",
        "        d1 = self.up1(d2, e1)    # 16x\n",
        "\n",
        "        u1 = self.up_learn1(d1)  # 32x\n",
        "        u2 = self.up_learn2(u1)  # 64x\n",
        "        u3 = self.up_learn3(u2)  # 128x\n",
        "\n",
        "        r128 = self.refine(self.refine_in(u3))\n",
        "        out128 = self.final_conv(r128)\n",
        "\n",
        "        # residual skip\n",
        "        up_input = F.interpolate(x, size=out128.shape[2:], mode='bilinear', align_corners=False)\n",
        "        out128 = out128 + up_input\n",
        "\n",
        "        if self.deep_supervision:\n",
        "            out32 = self.out32(u1)\n",
        "            out64 = self.out64(u2)\n",
        "            return out32, out64, out128\n",
        "        else:\n",
        "            return out128\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4zij_PllY40",
        "outputId": "be7a19af-1060-4f37-9c90-1c87dc179274"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "import lpips\n",
        "\n",
        "# -------------------------\n",
        "# Pixel loss\n",
        "# -------------------------\n",
        "pixel_crit = nn.MSELoss()\n",
        "\n",
        "# -------------------------\n",
        "# Perceptual (VGG) loss\n",
        "# -------------------------\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Expects inputs in [-1,1]. Internally maps to [0,1] and applies ImageNet mean/std.\n",
        "    Runs VGG feature extraction in float32 (even under autocast) for stability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES).features\n",
        "        self.slice1 = nn.Sequential(*list(vgg[:4])).eval()   # conv1_2\n",
        "        self.slice2 = nn.Sequential(*list(vgg[4:9])).eval()  # conv2_2\n",
        "        self.slice3 = nn.Sequential(*list(vgg[9:16])).eval() # conv3_3\n",
        "        for m in (self.slice1, self.slice2, self.slice3):\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # ImageNet norm buffers\n",
        "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
        "        self.register_buffer('std',  torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
        "\n",
        "    def _prep(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # [-1,1] -> [0,1] -> ImageNet norm\n",
        "        x01 = (x.clamp(-1,1) + 1) / 2\n",
        "        return (x01 - self.mean) / self.std\n",
        "\n",
        "    def forward(self, sr: torch.Tensor, hr: torch.Tensor) -> torch.Tensor:\n",
        "        # Force fp32 for VGG path even if outer training is mixed precision\n",
        "        sr32 = self._prep(sr).float()\n",
        "        hr32 = self._prep(hr).float()\n",
        "\n",
        "        f1_sr, f1_hr = self.slice1(sr32), self.slice1(hr32)\n",
        "        f2_sr, f2_hr = self.slice2(f1_sr),  self.slice2(f1_hr)\n",
        "        f3_sr, f3_hr = self.slice3(f2_sr),  self.slice3(f2_hr)\n",
        "\n",
        "        # Sum of MSEs across a few layers\n",
        "        return (F.mse_loss(f1_sr, f1_hr) +\n",
        "                F.mse_loss(f2_sr, f2_hr) +\n",
        "                F.mse_loss(f3_sr, f3_hr))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "perceptual_crit = VGGPerceptualLoss().to(device)\n",
        "\n",
        "def attention_loss(\n",
        "    sr, hr, heat,\n",
        "    *, gamma: float = 1.3,   # >1 = more focus on hot zones\n",
        "       floor: float = 0.10,  # weight outside the mask (0..1)\n",
        "       eps: float = 1e-6\n",
        "):\n",
        "    \"\"\"\n",
        "    Per-sample normalized masked MAE:\n",
        "      loss_i = sum(w_i * |sr - hr|) / sum(w_i), then averaged over the batch.\n",
        "    - sr, hr: (B, C, H, W) in [-1, 1]\n",
        "    - heat: (B, 1, H, W) or (B, H, W)\n",
        "    - gamma: selectivity of the heatmap\n",
        "    - floor: minimum gradient even outside the mask\n",
        "    \"\"\"\n",
        "    if heat is None:\n",
        "        return sr.new_tensor(0.0)\n",
        "\n",
        "    if heat.dim() == 3:\n",
        "        heat = heat.unsqueeze(1)  # (B,1,H,W)\n",
        "\n",
        "    heat = heat.to(device=sr.device, dtype=sr.dtype)\n",
        "    B = heat.size(0)\n",
        "\n",
        "    # min-max per campione -> [0,1]\n",
        "    flat = heat.reshape(B, -1)\n",
        "    hmin = flat.min(dim=1, keepdim=True)[0].reshape(B,1,1,1)\n",
        "    hmax = flat.max(dim=1, keepdim=True)[0].reshape(B,1,1,1)\n",
        "    span = (hmax - hmin)\n",
        "\n",
        "    hn = (heat - hmin) / span.clamp_min(eps)     # [0,1]\n",
        "    if abs(gamma - 1.0) > 1e-6:\n",
        "        hn = hn.clamp(0,1).pow(gamma)\n",
        "\n",
        "    # w' = floor + (1-floor)*hn  in [floor,1]\n",
        "    w = floor + (1.0 - floor) * hn\n",
        "\n",
        "    # se mappa ~costante, usa pesi uniformi (tutti 1)\n",
        "    uniform = (span <= eps)\n",
        "    if uniform.any():\n",
        "        w = torch.where(uniform, torch.ones_like(w), w)\n",
        "\n",
        "    # niente grad attraverso i pesi\n",
        "    w = w.expand_as(sr).detach()\n",
        "\n",
        "    # riduzione per-sample (reshape evita problemi di contiguità)\n",
        "    w_flat   = w.reshape(B, -1)\n",
        "    mae_flat = (w * (sr - hr).abs()).reshape(B, -1)\n",
        "    loss_per_sample = mae_flat.sum(dim=1) / w_flat.sum(dim=1).clamp_min(eps)\n",
        "    return loss_per_sample.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# LPIPS loss\n",
        "# -------------------------\n",
        "# lpips expects inputs in [-1,1]; returns (B,1,1,1) or (B,)\n",
        "lpips_crit = lpips.LPIPS(net='vgg').to(device)\n",
        "\n",
        "def lpips_loss(sr, hr):\n",
        "    return lpips_crit(sr, hr).mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "U0-5vDrLhyKh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "from torchmetrics.image.ssim import MultiScaleStructuralSimilarityIndexMeasure as MSSSIMMetric\n",
        "\n",
        "def _area_resize(x, size_hw):\n",
        "    if x is None: return None\n",
        "    if x.shape[-2:] == size_hw: return x\n",
        "    return F.interpolate(x, size=size_hw, mode=\"area\")\n",
        "\n",
        "def _renorm_heat(h):\n",
        "    if h is None: return None\n",
        "    m = h.amax(dim=(-2, -1), keepdim=True).clamp_min(1e-6)\n",
        "    return h / m\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg    = cfg\n",
        "        self.G = SuperResolutionUNet(\n",
        "            in_channels   = cfg.get('in_channels', 3),\n",
        "            base_filters  = cfg.get('base_filters', 32),\n",
        "            out_channels  = cfg.get('out_channels', 3),\n",
        "            refine_blocks = cfg.get('refine_blocks', 3)\n",
        "        ).to(device)\n",
        "        self.optG   = torch.optim.Adam(self.G.parameters(), lr=cfg['lr_g'])\n",
        "        self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "        self.metric_stride = 5\n",
        "\n",
        "        self.msssim_metric = MSSSIMMetric(\n",
        "            data_range=2.0,\n",
        "            kernel_size=(7, 7),\n",
        "            betas=(0.0448, 0.2856, 0.3001, 0.2363),\n",
        "            normalize=\"relu\",\n",
        "        ).to(device)\n",
        "\n",
        "        self.ms_scales  = [1.0, 0.5, 0.25]\n",
        "        self.ms_weights = [1.0, 0.2, 0.05]\n",
        "        assert len(self.ms_scales) == len(self.ms_weights)\n",
        "\n",
        "        self.perc_min_side  = int(cfg.get(\"ms_perc_min_side\",  96))\n",
        "        self.lpips_min_side = int(cfg.get(\"ms_lpips_min_side\", 96))\n",
        "\n",
        "    @staticmethod\n",
        "    def psnr_from_mse(mse, max_val=2.0):\n",
        "        return 10.0 * torch.log10((max_val * max_val) / (mse + 1e-8))\n",
        "\n",
        "    def compute_overall_metrics(self, sr, hr):\n",
        "        with torch.no_grad():\n",
        "            mse_overall = ((sr - hr) ** 2).mean(dim=(1,2,3))\n",
        "            psnr_overall   = self.psnr_from_mse(mse_overall).mean().item()\n",
        "            ssim_overall   = float(ssim(sr, hr, data_range=2.0))\n",
        "            msssim_overall = float(self.msssim_metric(sr, hr).cpu())\n",
        "            self.msssim_metric.reset()\n",
        "            lpips_overall  = lpips_loss(sr, hr)\n",
        "            if torch.is_tensor(lpips_overall):\n",
        "                lpips_overall = lpips_overall.mean().item()\n",
        "            return {\n",
        "                'psnr_overall': psnr_overall,\n",
        "                'ssim_overall': ssim_overall,\n",
        "                'msssim_overall': msssim_overall,\n",
        "                'lpips_overall': lpips_overall\n",
        "            }\n",
        "\n",
        "    def _build_ms_pairs(self, out32, out64, out128, x2, x4, hr, heat):\n",
        "        def flags(h, w):\n",
        "            return (min(h, w) >= self.perc_min_side), (min(h, w) >= self.lpips_min_side)\n",
        "\n",
        "        allow_perc_128, allow_lp_128 = flags(*hr.shape[-2:])\n",
        "        allow_perc_64,  allow_lp_64  = flags(*x4.shape[-2:])\n",
        "        allow_perc_32,  allow_lp_32  = flags(*x2.shape[-2:])\n",
        "\n",
        "        heat128 = _renorm_heat(heat)\n",
        "        heat64  = _renorm_heat(_area_resize(heat, (64, 64))) if heat is not None else None\n",
        "        heat32  = _renorm_heat(_area_resize(heat, (32, 32))) if heat is not None else None\n",
        "\n",
        "        sw = dict(zip(self.ms_scales, self.ms_weights))\n",
        "        return [\n",
        "            ('1x', out128, hr,   heat128, allow_perc_128, allow_lp_128, sw.get(1.0, 0.0)),\n",
        "            ('x4', out64,  x4,   heat64,  allow_perc_64,  allow_lp_64,  sw.get(0.5, 0.0)),\n",
        "            ('x2', out32,  x2,   heat32,  allow_perc_32,  allow_lp_32,  sw.get(0.25, 0.0)),\n",
        "        ]\n",
        "\n",
        "    def _multiscale_loss_from_pairs(self, ms_pairs, epoch=None, scheduler=None):\n",
        "        use_perc  = self.cfg.get('use_perc',  False)\n",
        "        use_attn  = self.cfg.get('use_attn',  False)\n",
        "        use_lpips = self.cfg.get('use_lpips', False)\n",
        "\n",
        "        if epoch is not None and scheduler is not None:\n",
        "            scale_perc, scale_attn, scale_lp = scheduler.scales_at(epoch, self.cfg)\n",
        "        else:\n",
        "            scale_perc, scale_attn, scale_lp = 1.0, 1.0, 1.0\n",
        "\n",
        "        Lpix = 0.0; Lperc = 0.0; Lattn = 0.0; Llp = 0.0\n",
        "        for tag, pred, tgt_img, tgt_heat, allow_perc, allow_lp, w in ms_pairs:\n",
        "            if w <= 0.0: continue\n",
        "            Lpix += w * pixel_crit(pred, tgt_img)\n",
        "            if use_attn and tgt_heat is not None:\n",
        "                Lattn += w * attention_loss(pred, tgt_img, tgt_heat)\n",
        "            if use_perc and allow_perc:\n",
        "                Lperc += w * perceptual_crit(pred, tgt_img)\n",
        "            if use_lpips and allow_lp:\n",
        "                Llp += w * lpips_loss(pred, tgt_img)\n",
        "\n",
        "        loss = Lpix \\\n",
        "            + self.cfg.get('w_perc', 0.0)  * (scale_perc * Lperc if torch.is_tensor(Lperc) else 0.0) \\\n",
        "            + self.cfg.get('w_attn', 0.0)  * (scale_attn * Lattn if torch.is_tensor(Lattn) else 0.0) \\\n",
        "            + self.cfg.get('w_lpips', 0.0) * (scale_lp   * Llp   if torch.is_tensor(Llp)   else 0.0)\n",
        "        return Lpix, Lperc, Lattn, Llp, loss\n",
        "\n",
        "    def train_epoch(self, loader, epoch=None, scheduler=None):\n",
        "        agg = {k: 0.0 for k in [\n",
        "            'loss_pixel','loss_perc','loss_attn','loss_lpips','loss_combined',\n",
        "            'psnr_overall','ssim_overall','msssim_overall','lpips_overall'\n",
        "        ]}\n",
        "        self.G.train()\n",
        "        step = metric_steps = 0\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        for x2, x4, hr, lr, heat in tqdm(loader, desc=f\"Training {self.cfg['name']}\"):\n",
        "            lr, hr, x4, x2 = (x.to(device, non_blocking=True) for x in [lr, hr, x4, x2])\n",
        "            heat = heat.to(device, non_blocking=True) if self.cfg.get('use_attn', False) else None\n",
        "\n",
        "            self.optG.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type='cuda', enabled=use_cuda, dtype=torch.float16):\n",
        "                out32, out64, out128 = self.G(lr)\n",
        "                ms_pairs = self._build_ms_pairs(out32, out64, out128, x2, x4, hr, heat)\n",
        "                Lpix, Lperc, Lattn, Llp, loss = self._multiscale_loss_from_pairs(ms_pairs, epoch, scheduler)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optG)\n",
        "            self.scaler.update()\n",
        "\n",
        "            agg['loss_pixel']    += float(Lpix.detach())\n",
        "            agg['loss_perc']     += float(Lperc.detach()) if self.cfg.get('use_perc', False) else 0.0\n",
        "            agg['loss_attn']     += float(Lattn.detach()) if self.cfg.get('use_attn', False) else 0.0\n",
        "            agg['loss_lpips']    += float(Llp.detach())   if self.cfg.get('use_lpips', False) else 0.0\n",
        "            agg['loss_combined'] += float(loss.detach())\n",
        "\n",
        "            if step % self.metric_stride == 0:\n",
        "                m = self.compute_overall_metrics(out128.detach(), hr)\n",
        "                for k in ('psnr_overall','ssim_overall','msssim_overall','lpips_overall'):\n",
        "                    agg[k] += m[k]\n",
        "                metric_steps += 1\n",
        "            step += 1\n",
        "\n",
        "        return {k: (v / max(step,1) if \"loss\" in k else v / max(metric_steps,1)) for k, v in agg.items()}\n",
        "\n",
        "    def evaluate(self, loader, num_samples=500, epoch=None, scheduler=None):\n",
        "        self.G.eval()\n",
        "        agg = {k: 0.0 for k in [\n",
        "            'loss_pixel','loss_perc','loss_attn','loss_lpips','loss_combined',\n",
        "            'psnr_overall','ssim_overall','msssim_overall','lpips_overall'\n",
        "        ]}\n",
        "        n = 0\n",
        "        with torch.no_grad():\n",
        "            for x2, x4, hr, lr, heat in tqdm(loader, desc=f\"Evaluating {self.cfg['name']}\"):\n",
        "                if n >= num_samples: break\n",
        "                lr, hr, x4, x2 = (x.to(device, non_blocking=True) for x in [lr, hr, x4, x2])\n",
        "                heat = heat.to(device, non_blocking=True) if self.cfg.get('use_attn', False) else None\n",
        "\n",
        "                out32, out64, out128 = self.G(lr)\n",
        "                m = self.compute_overall_metrics(out128, hr)\n",
        "                for k in ('psnr_overall','ssim_overall','msssim_overall','lpips_overall'):\n",
        "                    agg[k] += m[k]\n",
        "\n",
        "                ms_pairs = self._build_ms_pairs(out32, out64, out128, x2, x4, hr, heat)\n",
        "                Lpix, Lperc, Lattn, Llp, Lcomb = self._multiscale_loss_from_pairs(ms_pairs, epoch, scheduler)\n",
        "\n",
        "                agg['loss_pixel']    += float(Lpix)\n",
        "                agg['loss_perc']     += float(Lperc)  if self.cfg.get('use_perc', False)  else 0.0\n",
        "                agg['loss_attn']     += float(Lattn)  if self.cfg.get('use_attn', False)  else 0.0\n",
        "                agg['loss_lpips']    += float(Llp)    if self.cfg.get('use_lpips', False) else 0.0\n",
        "                agg['loss_combined'] += float(Lcomb)\n",
        "                n += 1\n",
        "\n",
        "        return {k: (v / max(n, 1)) for k, v in agg.items()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "fR8am-MfnEYK",
        "outputId": "dc7c2309-1451-4335-ebee-300e29ef1ef7"
      },
      "outputs": [],
      "source": [
        "# --- core imports\n",
        "import os, h5py, numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "import h5py\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# --- device & seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pin = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- paths & basic hyperparams\n",
        "data_path  = path            # <- set this to your CelebA root\n",
        "heat_h5    = './heatmaps.h5' # <- your heatmaps file\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "ckpt_path = None\n",
        "# Uncomment this and the other part in the code if loading a pretrained version of the model\n",
        "#ckpt_path = \"./PATH_TO_CHECKPOINT.pt\"\n",
        "\n",
        "with h5py.File(heat_h5, 'r') as f:\n",
        "    HM_COUNT = int(f['heatmaps'].shape[0])\n",
        "print(\"Heatmaps available:\", HM_COUNT)\n",
        "\n",
        "TRAIN_LIMIT = HM_COUNT                        # train limited by heatmaps (check not needed but assure consistency and prevents errors)\n",
        "VAL_LIMIT   = max(1, int(0.2 * TRAIN_LIMIT))  # 20% of train (if we have 10k training samples, we limit our validation to be 2000)\n",
        "\n",
        "# --- build datasets\n",
        "train_ds = CelebDataSet(data_path, 'train', heatmap_h5=heat_h5)\n",
        "val_ds   = CelebDataSet(data_path, 'val',   heatmap_h5=heat_h5)\n",
        "\n",
        "train_n = min(TRAIN_LIMIT, len(train_ds))\n",
        "val_n   = min(VAL_LIMIT,   len(val_ds))\n",
        "\n",
        "print(f\"Using TRAIN_LIMIT={train_n}, VAL_LIMIT={val_n}\")\n",
        "\n",
        "train_subset = Subset(train_ds, range(train_n))\n",
        "val_subset   = Subset(val_ds,   range(val_n))\n",
        "\n",
        "# --- dataloaders\n",
        "loader     = DataLoader(train_subset, batch_size=batch_size, shuffle=True,\n",
        "                        num_workers=2, pin_memory=pin)\n",
        "val_loader = DataLoader(val_subset,   batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=2, pin_memory=pin)\n",
        "\n",
        "print(\"Train samples:\", len(train_subset))\n",
        "print(\"Val samples:  \", len(val_subset))\n",
        "\n",
        "# --- one fixed batch for viz\n",
        "# This batch will be used to visualize validation results during validation process in the training loop\n",
        "data_iter = iter(loader)\n",
        "_, _, hr_f, lr_f, heat_f = next(data_iter)\n",
        "lr_vis, hr_vis, heat_vis = [t.to(device, non_blocking=True) for t in (lr_f[0:1], hr_f[0:1], heat_f[0:1])]\n",
        "\n",
        "# --- single config (you can add more later)\n",
        "configs = [\n",
        "    # This commented is the model that worked better. Take it as reference for changes.\n",
        "    dict(\n",
        "        name='deep_supervision_30k',\n",
        "        use_attn=True, use_perc=True, use_lpips=True,\n",
        "        w_attn=2.0, w_perc=0.01, w_lpips=0.15,\n",
        "        lr_g=1e-4,\n",
        "        base_filters=48,\n",
        "        refine_blocks=5\n",
        "    ),\n",
        "]\n",
        "\n",
        "for cfg in configs:\n",
        "    for k in ('w_perc','w_attn','w_lpips'):\n",
        "        cfg.setdefault(k, 0.0)\n",
        "        cfg.setdefault(f'_base_{k}', cfg[k])\n",
        "    cfg.setdefault('use_perc', False)\n",
        "    cfg.setdefault('use_attn', False)\n",
        "    cfg.setdefault('use_lpips', False)\n",
        "    cfg.setdefault('schedule', True)\n",
        "\n",
        "# --- build trainers from configs\n",
        "trainers = {}\n",
        "for cfg in configs:\n",
        "    t = Trainer(cfg)\n",
        "    if 'metric_stride' in cfg:\n",
        "        t.metric_stride = cfg['metric_stride']\n",
        "    t.lr_sched = ReduceLROnPlateau(t.optG, mode='min', patience=6, factor=0.5, verbose=True)\n",
        "    trainers[cfg['name']] = t\n",
        "\n",
        "# ==== helpers ===============================================================\n",
        "# Dynamically adjusts the weights of perceptual, attention, and LPIPS losses\n",
        "# throughout training based on the current epoch fraction.\n",
        "#\n",
        "# The schedule is divided into three phases:\n",
        "#   1. Warm-up (0–30% of total epochs): Gradually increases perceptual and attention\n",
        "#      loss weights from their starting values; LPIPS loss remains off.\n",
        "#   2. Mid-phase (30–85%): Slowly ramps perceptual and LPIPS weights to target values,\n",
        "#      and increases attention weight further.\n",
        "#   3. Late-phase (85–100%): Keeps perceptual weight fixed, slightly decreases attention\n",
        "#      weight, and slightly boosts LPIPS weight to emphasize fine texture details.\n",
        "#\n",
        "# If a given loss type is disabled in `cfg`, its weight is kept at 0 for the whole schedule.\n",
        "# 2) Scheduler che produce SCALE, poi moltiplica per i target base\n",
        "class LossWeightScheduler:\n",
        "    def __init__(self, num_epochs):\n",
        "        self.E = num_epochs\n",
        "\n",
        "    def scales_at(self, epoch, cfg):\n",
        "        \"\"\"Ritorna scale (0..1) per perc/attn/lpips, NON pesi assoluti.\"\"\"\n",
        "        t = epoch / self.E  # 0..1\n",
        "\n",
        "        if t <= 0.30:\n",
        "            u = (t / 0.30)\n",
        "            s_perc  = 0.00 + 1.00 * u      # 0 -> 1.00 del target base\n",
        "            s_attn  = 0.60 + 0.40 * u      # 0.60 -> 1.00 del target base\n",
        "            s_lpips = 0.00                 # off\n",
        "        elif t <= 0.85:\n",
        "            u = (t - 0.30) / 0.55\n",
        "            s_perc  = 1.00                 # già pieno target\n",
        "            s_attn  = 1.00                 # già pieno target\n",
        "            s_lpips = 0.00 + 1.00 * u      # 0 -> 1.00 del target base\n",
        "        else:\n",
        "            u = (t - 0.85) / 0.15\n",
        "            s_perc  = 1.00\n",
        "            s_attn  = 1.00 - 0.25 * u      # cala al 75% del target base\n",
        "            s_lpips = 1.00                 # pieno target\n",
        "        return s_perc, s_attn, s_lpips\n",
        "\n",
        "    def apply(self, epoch, cfg):\n",
        "        if not cfg.get('schedule', True):\n",
        "            # non toccare nulla\n",
        "            return\n",
        "        use_perc  = cfg.get('use_perc',  False)\n",
        "        use_attn  = cfg.get('use_attn',  False)\n",
        "        use_lpips = cfg.get('use_lpips', False)\n",
        "\n",
        "        s_perc, s_attn, s_lpips = self.scales_at(epoch, cfg)\n",
        "\n",
        "        base_perc  = cfg.get('_base_w_perc',  0.0)\n",
        "        base_attn  = cfg.get('_base_w_attn',  0.0)\n",
        "        base_lpips = cfg.get('_base_w_lpips', 0.0)\n",
        "\n",
        "        cfg['w_perc']  = (base_perc  * s_perc)  if use_perc  else 0.0\n",
        "        cfg['w_attn']  = (base_attn  * s_attn)  if use_attn  else 0.0\n",
        "        cfg['w_lpips'] = (base_lpips * s_lpips) if use_lpips else 0.0\n",
        "\n",
        "\n",
        "# Monitors a validation score and triggers early stopping when it stops improving.\n",
        "#\n",
        "# Parameters:\n",
        "#   patience  – number of consecutive epochs without significant improvement\n",
        "#               (greater than `min_delta`) before stopping.\n",
        "#   min_delta – minimum required improvement in the monitored score to be considered progress.\n",
        "#\n",
        "# Behavior:\n",
        "#   - Tracks the best (lowest) score seen so far.\n",
        "#   - Resets the bad epoch counter when improvement is detected.\n",
        "#   - Increments the bad epoch counter otherwise.\n",
        "#   - Sets `should_stop=True` when the bad epoch counter reaches `patience`.\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=1e-4):\n",
        "        self.patience=patience; self.min_delta=min_delta\n",
        "        self.best=None; self.bad_epochs=0; self.should_stop=False\n",
        "    def step(self, score):\n",
        "        if self.best is None or score < self.best - self.min_delta:\n",
        "            self.best=score; self.bad_epochs=0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            if self.bad_epochs >= self.patience: self.should_stop=True\n",
        "\n",
        "# Computes a scalar \"validation objective\" score to guide model selection,\n",
        "# learning rate scheduling, and early stopping. Lower is better.\n",
        "#\n",
        "# The score is a weighted sum of penalties:\n",
        "#   - 1.5 * LPIPS: strong penalty for poor perceptual similarity (higher LPIPS).\n",
        "#   - 0.8 * max(0, 0.02 - SSIM): penalty if SSIM falls below 0.02 (no penalty otherwise).\n",
        "#   - 0.2 * max(0, 20.0 - PSNR): penalty if PSNR is below 20 dB (no penalty otherwise).\n",
        "#\n",
        "# Inputs:\n",
        "#   m – dictionary of validation metrics containing:\n",
        "#       'ssim_overall', 'lpips_overall', 'psnr_overall'\n",
        "#\n",
        "# Output:\n",
        "#   A single float representing the validation objective; smaller values indicate better quality.\n",
        "def val_objective(m):\n",
        "    ssim = float(m.get('ssim_overall', 0.0))\n",
        "    lp   = float(m.get('lpips_overall', 1.0))\n",
        "    psnr = float(m.get('psnr_overall', 0.0))\n",
        "    return (1.5*lp) + (0.8*max(0.0, 0.02-ssim)) + (0.2*max(0.0, 20.0-psnr))\n",
        "\n",
        "def to01(x): return (x.clamp(-1,1) + 1)/2\n",
        "\n",
        "def save_checkpoint(trainer, history, epoch, best_score, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': trainer.G.state_dict(),\n",
        "        'opt': trainer.optG.state_dict(),\n",
        "        'scaler': trainer.scaler.state_dict(),\n",
        "        'sched': trainer.lr_sched.state_dict() if hasattr(trainer, 'lr_sched') else None,\n",
        "        'early_best': best_score,\n",
        "        'history': history,\n",
        "    }, path)\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "loss_sched = LossWeightScheduler(num_epochs)\n",
        "early_stop = EarlyStopping(patience=15, min_delta=1e-4)\n",
        "history    = {}\n",
        "start_epoch = 1\n",
        "\n",
        "\n",
        "if ckpt_path is not None:\n",
        "  name = configs[0]['name']\n",
        "  ckpt = torch.load(ckpt_path, map_location=device)\n",
        "  trainers[name].G.load_state_dict(ckpt['model'])\n",
        "  trainers[name].optG.load_state_dict(ckpt['opt'])\n",
        "  trainers[name].scaler.load_state_dict(ckpt['scaler'])\n",
        "  if ckpt.get('sched'):\n",
        "      trainers[name].lr_sched.load_state_dict(ckpt['sched'])\n",
        "  print(\"Loaded configuration from\", ckpt_path)\n",
        "\n",
        "  early_stop.best = ckpt.get('early_best', float('inf'))\n",
        "  trainers[name]._best_score = early_stop.best\n",
        "  start_epoch = ckpt.get('epoch', 0) + 1\n",
        "\n",
        "  history = ckpt.get('history', {})\n",
        "\n",
        "last_round_epoch = 1\n",
        "# TRAINING LOOP\n",
        "for epoch in range(start_epoch, num_epochs+1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # update loss weights\n",
        "    for cfg in configs:\n",
        "        loss_sched.apply(epoch, cfg)\n",
        "\n",
        "    # ---- TRAIN ----\n",
        "    epoch_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name    = cfg['name']\n",
        "        metrics = trainers[name].train_epoch(loader)\n",
        "        epoch_metrics[name] = metrics\n",
        "\n",
        "        # history (train)\n",
        "        hist = history.setdefault(name, {})\n",
        "        for k, v in metrics.items():\n",
        "            hist.setdefault(k, []).append(v)\n",
        "\n",
        "    # print train (now with MS-SSIM)\n",
        "    headers = [\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\"]\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']; m = epoch_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['loss_pixel']:.4e}\",\n",
        "            f\"{m['loss_perc']:.4e}\"    if cfg.get('use_perc', False)  else \"-\",\n",
        "            f\"{m['loss_attn']:.4e}\"    if cfg.get('use_attn', False)  else \"-\",\n",
        "            f\"{m['loss_lpips']:.4e}\"   if cfg.get('use_lpips', False) else \"-\",\n",
        "            f\"{m['loss_combined']:.4e}\",\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['msssim_overall']:.4f}\",\n",
        "            f\"{m['lpips_overall']:.4f}\",\n",
        "        ])\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    val_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']\n",
        "        vm = trainers[name].evaluate(val_loader)\n",
        "        val_metrics[name] = vm\n",
        "\n",
        "        # history (val)\n",
        "        hist = history[name]\n",
        "        for k, v in vm.items():\n",
        "            hist.setdefault('val_' + k, []).append(v)\n",
        "\n",
        "    ref_name = configs[0]['name']\n",
        "    ref_val  = val_metrics[ref_name]\n",
        "    score    = val_objective(ref_val)\n",
        "\n",
        "    # schedulers + early stop\n",
        "    for cfg in configs:\n",
        "        trainers[cfg['name']].lr_sched.step(val_objective(val_metrics[cfg['name']]))\n",
        "    early_stop.step(score)\n",
        "\n",
        "    # print val (now with MS-SSIM)\n",
        "    headers = [\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\",\"val_obj\"]\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']; m = val_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['loss_pixel']:.4e}\",\n",
        "            f\"{m['loss_perc']:.4e}\"    if cfg.get('use_perc', False)  else \"-\",\n",
        "            f\"{m['loss_attn']:.4e}\"    if cfg.get('use_attn', False)  else \"-\",\n",
        "            f\"{m['loss_lpips']:.4e}\"   if cfg.get('use_lpips', False) else \"-\",\n",
        "            f\"{m['loss_combined']:.4e}\",\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['msssim_overall']:.4f}\",\n",
        "            f\"{m['lpips_overall']:.4f}\",\n",
        "            f\"{val_objective(m):.4f}\",\n",
        "        ])\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "\n",
        "    # quick viz\n",
        "    if epoch % 10 ==0:\n",
        "        val_iter = iter(val_loader)\n",
        "        _, _, hr_val_b, lr_val_b, _ = next(val_iter)\n",
        "        lr_val = lr_val_b[:5].to(device)\n",
        "        hr_val = hr_val_b[:5].cpu()\n",
        "\n",
        "        lr_up_val = to01(F.interpolate(lr_val, size=(128,128), mode='bilinear', align_corners=False)).cpu()\n",
        "        recon_val = {}\n",
        "        with torch.no_grad():\n",
        "            for cfg in configs:\n",
        "                nm = cfg['name']\n",
        "                sr_out = trainers[nm].G.eval()(lr_val)\n",
        "                if isinstance(sr_out, tuple):\n",
        "                    _, _, sr = sr_out\n",
        "                else:\n",
        "                    sr = sr_out\n",
        "                recon_val[nm] = to01(sr).cpu()\n",
        "\n",
        "        fig, axes = plt.subplots(5, 2 + len(configs), figsize=(4*(2+len(configs)), 20))\n",
        "        for i in range(5):\n",
        "            row = axes[i]\n",
        "            row[0].imshow(lr_up_val[i].permute(1,2,0)); row[0].set_title(\"LR ↑\"); row[0].axis('off')\n",
        "            row[1].imshow(to01(hr_val[i]).permute(1,2,0)); row[1].set_title(\"HR GT\"); row[1].axis('off')\n",
        "            for j, cfg in enumerate(configs, start=2):\n",
        "                nm = cfg['name']\n",
        "                row[j].imshow(recon_val[nm][i].permute(1,2,0)); row[j].set_title(nm); row[j].axis('off')\n",
        "        plt.suptitle(f\"Epoch {epoch} — Validation Reconstructions\", fontsize=16)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "    # save best + periodic\n",
        "    best_dir = f\"./{ref_name}\"; os.makedirs(best_dir, exist_ok=True)\n",
        "    best_path = os.path.join(best_dir, f\"{ref_name}_best.pth\")\n",
        "    if score <= getattr(trainers[ref_name], \"_best_score\", float(\"inf\")):\n",
        "        torch.save(trainers[ref_name].G.state_dict(), best_path)\n",
        "        trainers[ref_name]._best_score = score\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        for cfg in configs:\n",
        "            name   = cfg['name']\n",
        "            last_round_epoch = epoch\n",
        "            folder = f\"./{name}\"; os.makedirs(folder, exist_ok=True)\n",
        "            path_to_save   =  f\"./{name}/{name}_epoch{epoch:03d}.pt\"\n",
        "            save_checkpoint(trainers[name], history, epoch, early_stop.best,path_to_save)\n",
        "\n",
        "    if early_stop.should_stop:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch} (best val objective: {early_stop.best:.4f}).\")\n",
        "\n",
        "        # --- load the best checkpoint ---\n",
        "        best_ckpt = torch.load(best_path, map_location=device)\n",
        "        trainers[ref_name].G.load_state_dict(best_ckpt)\n",
        "\n",
        "        # --- recompute validation metrics for best model ---\n",
        "        best_metrics = trainers[ref_name].evaluate(val_loader)\n",
        "\n",
        "        print(\"\\n=== Best Model Validation Metrics ===\")\n",
        "        print(tabulate([[\n",
        "            ref_name,\n",
        "            f\"{best_metrics['loss_pixel']:.4e}\",\n",
        "            f\"{best_metrics['loss_perc']:.4e}\"    if configs[0].get('use_perc', False)  else \"-\",\n",
        "            f\"{best_metrics['loss_attn']:.4e}\"    if configs[0].get('use_attn', False)  else \"-\",\n",
        "            f\"{best_metrics['loss_lpips']:.4e}\"   if configs[0].get('use_lpips', False) else \"-\",\n",
        "            f\"{best_metrics['loss_combined']:.4e}\",\n",
        "            f\"{best_metrics['psnr_overall']:.2f}\",\n",
        "            f\"{best_metrics['ssim_overall']:.4f}\",\n",
        "            f\"{best_metrics['msssim_overall']:.4f}\",\n",
        "            f\"{best_metrics['lpips_overall']:.4f}\",\n",
        "            f\"{val_objective(best_metrics):.4f}\",\n",
        "        ]], headers=[\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\",\"val_obj\"], tablefmt=\"github\"))\n",
        "\n",
        "        # --- visualize best model ---\n",
        "        val_iter = iter(val_loader)\n",
        "        _, _, hr_val_b, lr_val_b, _ = next(val_iter)\n",
        "        lr_val = lr_val_b[:5].to(device)\n",
        "        hr_val = hr_val_b[:5].cpu()\n",
        "\n",
        "        lr_up_val = to01(F.interpolate(lr_val, size=(128,128), mode='bilinear', align_corners=False)).cpu()\n",
        "        recon_val = {}\n",
        "        with torch.no_grad():\n",
        "            for cfg in configs:\n",
        "                nm = cfg['name']\n",
        "                sr_out = trainers[nm].G.eval()(lr_val)\n",
        "                if isinstance(sr_out, tuple):\n",
        "                    _, _, sr = sr_out\n",
        "                else:\n",
        "                    sr = sr_out\n",
        "                recon_val[nm] = to01(sr).cpu()\n",
        "\n",
        "\n",
        "        fig, axes = plt.subplots(5, 2 + len(configs), figsize=(4*(2+len(configs)), 20))\n",
        "        for i in range(5):\n",
        "            row = axes[i]\n",
        "            row[0].imshow(lr_up_val[i].permute(1,2,0)); row[0].set_title(\"LR ↑\"); row[0].axis('off')\n",
        "            row[1].imshow(to01(hr_val[i]).permute(1,2,0)); row[1].set_title(\"HR GT\"); row[1].axis('off')\n",
        "            for j, cfg in enumerate(configs, start=2):\n",
        "                nm = cfg['name']\n",
        "                row[j].imshow(recon_val[nm][i].permute(1,2,0)); row[j].set_title(f\"{nm} (best)\"); row[j].axis('off')\n",
        "        plt.suptitle(f\"Best Model — Validation Reconstructions\", fontsize=16)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        # best metrics\n",
        "\n",
        "\n",
        "        break\n",
        "\n",
        "\n",
        "path_to_save   =  f\"./{name}/{name}_epoch{last_round_epoch:03d}.pt\"\n",
        "files.download(path_to_save)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hlz7gPA-QzYy",
        "outputId": "a70c4354-4595-499e-b0f8-3209599c7438"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pick the model you want to plot (you have only one in configs now)\n",
        "name = list(history.keys())[0]\n",
        "H = history[name]\n",
        "\n",
        "def _ema(values, alpha=0.9):\n",
        "    \"\"\"Simple EMA over a 1D list/array. Returns a list of same length.\"\"\"\n",
        "    if values is None or len(values) == 0:\n",
        "        return []\n",
        "    out = []\n",
        "    m = None\n",
        "    for v in values:\n",
        "        v = float(v)\n",
        "        m = v if (m is None) else alpha * m + (1 - alpha) * v\n",
        "        out.append(m)\n",
        "    return out\n",
        "\n",
        "def plot_metric(metric_key, title, ylabel=\"\", invert=False, ema_alpha=0.9, show_raw=True):\n",
        "    \"\"\"\n",
        "    Plot train/val curves with optional EMA smoothing.\n",
        "    - ema_alpha: None to disable EMA; else e.g. 0.9\n",
        "    - show_raw: whether to also draw raw curves (faint) alongside EMA\n",
        "    \"\"\"\n",
        "    train = H.get(metric_key, [])\n",
        "    val   = H.get(f\"val_{metric_key}\", [])\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "\n",
        "    # Train\n",
        "    if train:\n",
        "        if ema_alpha is not None:\n",
        "            tr_ema = _ema(train, alpha=ema_alpha)\n",
        "            plt.plot(tr_ema, label=f\"train (EMA {ema_alpha})\")\n",
        "            if show_raw:\n",
        "                plt.plot(train, label=\"train (raw)\", linestyle=\"--\", alpha=0.35)\n",
        "        else:\n",
        "            plt.plot(train, label=\"train\")\n",
        "\n",
        "    # Val\n",
        "    if val:\n",
        "        if ema_alpha is not None:\n",
        "            va_ema = _ema(val, alpha=ema_alpha)\n",
        "            plt.plot(va_ema, label=f\"val (EMA {ema_alpha})\")\n",
        "            if show_raw:\n",
        "                plt.plot(val, label=\"val (raw)\", linestyle=\"--\", alpha=0.35)\n",
        "        else:\n",
        "            plt.plot(val, label=\"val\")\n",
        "\n",
        "    if invert:\n",
        "        plt.gca().invert_yaxis()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel if ylabel else metric_key)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Losses ===\n",
        "plot_metric(\"loss_combined\", \"Combined Loss\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "plot_metric(\"loss_pixel\",    \"Pixel Loss (MSE)\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_perc\" in H:  plot_metric(\"loss_perc\",  \"Perceptual Loss\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_attn\" in H:  plot_metric(\"loss_attn\",  \"Attention Loss\",  ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_lpips\" in H: plot_metric(\"loss_lpips\", \"LPIPS Loss\",      ylabel=\"Loss\", ema_alpha=0.9)\n",
        "\n",
        "# === Image Quality Metrics ===\n",
        "plot_metric(\"psnr_overall\",   \"PSNR\",     ylabel=\"dB\",     ema_alpha=0.9)\n",
        "plot_metric(\"ssim_overall\",   \"SSIM\",     ylabel=\"Score\",  ema_alpha=0.9)\n",
        "plot_metric(\"msssim_overall\", \"MS-SSIM\",  ylabel=\"Score\",  ema_alpha=0.9)\n",
        "plot_metric(\"lpips_overall\",  \"LPIPS\",    ylabel=\"Distance\", invert=True, ema_alpha=0.9)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
