{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":37705,"sourceType":"datasetVersion","datasetId":29561,"isSourceIdPinned":false},{"sourceId":1060492,"sourceType":"datasetVersion","datasetId":587947},{"sourceId":505789,"sourceType":"modelInstanceVersion","modelInstanceId":401639,"modelId":419659}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"kevinpatel04/celeba-original-wild-images\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-11T14:11:44.065373Z","iopub.execute_input":"2025-08-11T14:11:44.065711Z","iopub.status.idle":"2025-08-11T14:14:26.566912Z","shell.execute_reply.started":"2025-08-11T14:11:44.065682Z","shell.execute_reply":"2025-08-11T14:14:26.565957Z"}},"outputs":[{"name":"stdout","text":"Mounting files to /kaggle/input/celeba-original-wild-images...\nPath to dataset files: /kaggle/input/celeba-original-wild-images\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip -qq install ultralytics","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T09:50:48.699009Z","iopub.execute_input":"2025-08-08T09:50:48.699317Z","iopub.status.idle":"2025-08-08T09:50:49.095472Z","shell.execute_reply.started":"2025-08-08T09:50:48.699294Z","shell.execute_reply":"2025-08-08T09:50:49.094753Z"}},"outputs":[{"name":"stdout","text":"Path to dataset files: /kaggle/input/celeba-dataset\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torchvision.transforms.functional as TF\nimport torchvision.transforms as transforms\nimport numpy as np, torch, cv2, os, glob\nimport matplotlib.pyplot as plt\nfrom ultralytics import YOLO\nfrom tqdm import tqdm\nfrom PIL import Image\n\n# ───────────────────────────────── CONFIG ─────────────────────────────────\nINPUT_DIR  = \"/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba\"\nMODEL_PATH = \"/kaggle/input/yolo-world-small/pytorch/default/1/yolov8s-world.pt\"\nMASK_H, MASK_W = 128, 128\n\nPROMPTS = ['eye', 'mouth', 'ear', 'nose', 'nosetip', 'face', 'chin', 'head', 'eyebrows']\nWEIGHTS = {\n    'eye': 4.5, 'mouth': 4.0, 'nose': 4.0, 'nosetip': 3.0,\n    'chin': 4.0, 'ear': 4.0, 'eyebrows': 4.0, 'face': 4.0, 'head': 1.2\n}\n\nNUM_IMAGES = 30000\n\n# ───────────────────────────────── UTILS ─────────────────────────────────\ndef inverse_gaussian_fade(shape):\n    hh, ww = shape\n    y, x = np.meshgrid(np.linspace(-1,1,ww), np.linspace(-1,1,hh))\n    distance = np.sqrt(x**2 + y**2)\n    sigma = 0.6\n    fade = 1 - np.exp(-(distance**2) / (2 * sigma**2))\n    return fade.astype(np.float32)\n\nINV_FADE_CACHE = {}\n\ndef get_inverse_fade(shape):\n    if shape not in INV_FADE_CACHE:\n        INV_FADE_CACHE[shape] = inverse_gaussian_fade(shape)\n    return INV_FADE_CACHE[shape]\n\ndef extract_edges(img_gray):\n    scharr_x = cv2.Scharr(img_gray, cv2.CV_32F, 1, 0)\n    scharr_y = cv2.Scharr(img_gray, cv2.CV_32F, 0, 1)\n    scharr = np.sqrt(scharr_x**2 + scharr_y**2)\n    scharr /= (scharr.max() + 1e-8)\n\n    canny = cv2.Canny(img_gray, 50, 150).astype(np.float32)/255.0\n\n    combined = (0.5 * scharr + 0.5 * canny)\n    return combined / (combined.max() + 1e-8)\n\ndef gaussian_fade(shape):\n    hh, ww = shape\n    y, x = np.meshgrid(np.linspace(-1,1,ww), np.linspace(-1,1,hh))\n    fade = np.exp(-2.0 * (x**2 + y**2))\n    return fade.astype(np.float32)\n\nFADE_CACHE = {}\n\ndef get_fade(shape):\n    if shape not in FADE_CACHE:\n        FADE_CACHE[shape] = gaussian_fade(shape)\n    return FADE_CACHE[shape]\n\ndef preprocess_exactly_like_dataset(img_path: str) -> np.ndarray:\n    # Carica immagine come PIL (come nel dataset)\n    pil_image = Image.open(img_path).convert('RGB')\n    \n    # Applica le STESSE trasformazioni del dataset (data_augmentation=False)\n    transform = transforms.Compose([\n        transforms.CenterCrop((178, 178)),    # IDENTICO al dataset\n        transforms.Resize((128, 128)),        # IDENTICO al dataset\n    ])\n    \n    # Applica trasformazione\n    processed_pil = transform(pil_image)\n    \n    # Converti a numpy array (RGB, non BGR!)\n    img_rgb = np.array(processed_pil)\n    \n    return img_rgb\n\ndef detailed_heatmap_aligned(img_path, yolo_model, prompts, weights):\n    # 1. Preprocessa ESATTAMENTE come il dataset PyTorch\n    img128 = preprocess_exactly_like_dataset(img_path)\n    H, W = 128, 128  # Dimensioni finali\n    \n    # 2. Applica YOLO sull'immagine preprocessata (128x128)\n    gray128 = cv2.cvtColor(img128, cv2.COLOR_RGB2GRAY)\n    \n    results = yolo_model(img128, conf=0.001, iou=0.1, verbose=False)[0]\n    \n    if len(results.boxes) == 0:\n        return np.zeros((H, W), dtype=np.float32)\n    \n    boxes = results.boxes.xyxy.cpu().numpy().astype(int)\n    classes = results.boxes.cls.cpu().numpy().astype(int)\n    confidences = results.boxes.conf.cpu().numpy()\n\n    combined_heatmap = np.zeros((H, W), dtype=np.float32)\n\n    # 3. Applica la logica di generazione heatmap (invariata)\n    for (x1, y1, x2, y2), cls, conf in zip(boxes, classes, confidences):\n        lab = prompts[cls]\n        weight = weights.get(lab, 1.0)\n\n        # Assicurati che le coordinate siano valide\n        x1, y1 = max(0, x1), max(0, y1)\n        x2, y2 = min(W, x2), min(H, y2)\n        \n        if x2 <= x1 or y2 <= y1:\n            continue\n\n        crop = gray128[y1:y2, x1:x2]\n        if crop.size == 0:\n            continue\n\n        # Estrai edges dalla crop\n        edges = extract_edges(crop)\n        \n        # Crea una maschera più grande per evitare bordi netti\n        box_w, box_h = x2 - x1, y2 - y1\n        pad = max(8, min(box_w, box_h) // 4)  # Padding adattivo\n        \n        # Espandi la regione con padding\n        y1_exp = max(0, y1 - pad)\n        y2_exp = min(H, y2 + pad)\n        x1_exp = max(0, x1 - pad)\n        x2_exp = min(W, x2 + pad)\n        \n        # Crea una heatmap locale più grande\n        local_h, local_w = y2_exp - y1_exp, x2_exp - x1_exp\n        local_heatmap = np.zeros((local_h, local_w), dtype=np.float32)\n        \n        # Posiziona gli edges nella regione centrale\n        crop_start_y = y1 - y1_exp\n        crop_start_x = x1 - x1_exp\n        \n        # Resize edges per matchare la box originale\n        edges_resized = cv2.resize(edges, (box_w, box_h), interpolation=cv2.INTER_LINEAR)\n        \n        # FIX: Applica fade in modo più intelligente\n        if lab == 'head':\n            # Per head: usa normal fade (non inverse) e peso ridotto\n            edges_resized *= get_fade(edges_resized.shape)\n            weight *= 0.5  # Ulteriore riduzione per head\n        elif lab == 'face':\n            # Per face: inverse fade ma più contenuto\n            fade = get_inverse_fade(edges_resized.shape)\n            fade = 0.3 + 0.7 * fade  # Riduce l'effetto inverse\n            edges_resized *= fade\n        else:\n            # Per altri landmark: normal fade\n            edges_resized *= get_fade(edges_resized.shape)\n        \n        # Inserisci nella heatmap locale\n        local_heatmap[crop_start_y:crop_start_y + box_h, crop_start_x:crop_start_x + box_w] = edges_resized\n        \n        # Applica un fade radiale più ampio per coprire tutta la bounding box\n        center_y, center_x = local_h // 2, local_w // 2\n        y_coords, x_coords = np.ogrid[:local_h, :local_w]\n        \n        # Distanza dal centro\n        distances = np.sqrt((x_coords - center_x)**2 + (y_coords - center_y)**2)\n        max_dist = min(local_h, local_w) / 2\n        \n        # Fade più ampio e meno aggressivo per coprire meglio la bounding box\n        radial_fade = np.exp(-0.8 * (distances / max_dist)**2)\n        radial_fade = np.clip(radial_fade, 0.3, 1.0)  # Baseline più alto per coprire più area\n        \n        local_heatmap *= radial_fade\n        \n        # Aggiungi alla heatmap globale con blending\n        combined_heatmap[y1_exp:y2_exp, x1_exp:x2_exp] = np.maximum(\n            combined_heatmap[y1_exp:y2_exp, x1_exp:x2_exp], \n            local_heatmap * weight\n        )\n\n    # 4. Post-processing più aggressivo per smoothing\n    combined_heatmap = cv2.GaussianBlur(combined_heatmap, (21, 21), sigmaX=2.0)\n    \n    if combined_heatmap.max() > combined_heatmap.min():\n        combined_heatmap = (combined_heatmap - combined_heatmap.min()) / (combined_heatmap.max() - combined_heatmap.min())\n    \n    return combined_heatmap\n\ndef verify_alignment_with_dataset(img_path, dataset_loader_func=None):\n    \"\"\"\n    BONUS: Funzione per verificare l'allineamento perfetto\n    \"\"\"\n    # Metodo 1: Il nostro preprocessing\n    our_img = preprocess_exactly_like_dataset(img_path)\n    \n    # Metodo 2: Se hai una funzione del dataset, confronta\n    if dataset_loader_func:\n        dataset_img = dataset_loader_func(img_path)\n        \n        # Verifica se sono identici\n        diff = np.abs(our_img.astype(float) - dataset_img.astype(float))\n        max_diff = diff.max()\n        \n        print(f\"✅ Alignment check: max pixel difference = {max_diff}\")\n        if max_diff < 1e-6:\n            print(\"PERFECT ALIGNMENT!\")\n        elif max_diff < 1.0:\n            print(\"Very good alignment (sub-pixel differences)\")\n        else:\n            print(\"Significant differences detected\")\n    \n    return our_img\n\n# ───────────────────────────────── SETUP ─────────────────────────────────\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nyolo_model = YOLO(MODEL_PATH).to(device)\nyolo_model.set_classes(PROMPTS)\nyolo_model.eval()\n\nimage_paths = sorted(glob.glob(os.path.join(INPUT_DIR, '*.jpg')))[:NUM_IMAGES]\n\n# ───────────────────────────────── GENERATE PIXEL-PERFECT ALIGNED HEATMAPS ─────────────────────────────────\nheatmaps = []\nprocessed_images = []\n\nprint(\"Generating pixel-perfect aligned heatmaps...\")\n\nfor path in tqdm(image_paths, desc=\"Extracting aligned heatmaps\"):\n    \n    # FIXED: Usa il preprocessing identico al dataset\n    img128 = preprocess_exactly_like_dataset(path)\n    \n    # Genera heatmap con allineamento perfetto\n    hm = detailed_heatmap_aligned(path, yolo_model, PROMPTS, WEIGHTS)\n    \n    heatmaps.append(hm)\n    processed_images.append(img128)\n\nheatmaps = np.stack(heatmaps)  # (N,128,128)\nprocessed_images = np.stack(processed_images)  # (N,128,128,3)\n\n# ───────────────────────────────── SAVE ─────────────────────────────────\nimport h5py\n\nwith h5py.File('heatmaps_aligned_10k.h5', 'w') as hf:\n    hf.create_dataset(\n        'heatmaps', data=heatmaps.astype('f2'),\n        compression='gzip', compression_opts=4\n    )\n    hf.create_dataset(\n        'images', data=processed_images.astype('uint8'),\n        compression='gzip', compression_opts=4\n    )\n\nprint(f\"Saved {len(heatmaps)} PIXEL-PERFECT aligned heatmaps to heatmaps_pixel_perfect_aligned.h5\")\nprint(\"Heatmaps are now perfectly aligned with CelebDataSet (data_augmentation=False)!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}