{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hVilGaX2n02x",
        "outputId": "f5896f4a-119f-4b97-c9ce-b5facc10dd6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.4/363.4 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:31\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qq ultralytics\n",
        "!pip install -qq torchmetrics\n",
        "!pip install -qq lpips"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ABDtlwOcFvU"
      },
      "source": [
        "### Dataset and Heatmap Download\n",
        "\n",
        "This block performs the following steps:\n",
        "\n",
        "1. **Download CelebA Dataset**  \n",
        "   Uses `kagglehub` to fetch the *CelebA* dataset (`jessicali9530/celeba-dataset`)  \n",
        "   and prints the local download path.\n",
        "\n",
        "2. **Set PyTorch Device**  \n",
        "   Detects if a GPU is available and sets `torch.device` to `\"cuda\"` or `\"cpu\"` accordingly.\n",
        "\n",
        "3. **Download Precomputed Heatmaps**  \n",
        "   Retrieves an `.h5` file containing precomputed heatmaps from Hugging Face.  \n",
        "   - Multiple dataset sizes are available:\n",
        "     - **10k** samples (small, for quick tests)\n",
        "     - **30k** samples (medium, balanced)\n",
        "     - **50k** samples (full set)  \n",
        "   - !!!Only one `curl` command should be uncommented at a time!!!\n",
        "\n",
        "4. **Purpose**  \n",
        "   The CelebA images serve as the main training data,  \n",
        "   while the heatmaps (e.g., facial landmark or attention maps)  \n",
        "   are used as auxiliary inputs or for attention-based loss functions in the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhQkrGmuneWY"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "#dowload heatmaps\n",
        "#small 10k\n",
        "!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps_10k.h5\n",
        "\n",
        "#medium 30k\n",
        "#curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps_30k.h5\n",
        "\n",
        "#uncomment the following line to use the 50k version of the dataset\n",
        "#!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzwnTm3gcZ8R"
      },
      "source": [
        "### CelebDataSet: CelebA + (optional) facial landmark heatmaps\n",
        "\n",
        "This `torch.utils.data.Dataset` builds a **progressive, multi-scale** face SR pipeline (16→32→64→128) and can optionally load **landmark heatmaps** from an HDF5 file. It mirrors the common setup in progressive face super‑resolution (e.g., Kim et al., *Progressive Face Super-Resolution via Attention to Facial Landmark*, 2019).\n",
        "\n",
        "**What it returns (per sample)**\n",
        "- `x2` — 32×32 target tensor  \n",
        "- `x4` — 64×64 target tensor  \n",
        "- `hr` — 128×128 “high‑res” target tensor  \n",
        "- `lr` — 16×16 low‑res input tensor  \n",
        "- `heatmap` — (1×128×128) float tensor with facial landmark attention (zeros if not provided)\n",
        "\n",
        "**How it works**\n",
        "1. **Splits**  \n",
        "   Reads `list_eval_partition.csv` to build train/val/test file lists, then picks the split via `state={'train','val','test'}`.\n",
        "\n",
        "2. **Preprocessing & Augmentation**  \n",
        "   - Center-crop to 178×178 → resize to 128×128.  \n",
        "   - Optional training‑time aug: horizontal flip, ±20° rotation (bilinear), color jitter.  \n",
        "   - Normalize to `[-1, 1]` per channel with `transforms.Normalize((0.5,)*3, (0.5,)*3)`.\n",
        "\n",
        "3. **Multi‑scale pyramid**  \n",
        "   From the 128×128 image, it creates:\n",
        "   - `x4`: 64×64  \n",
        "   - `x2`: 32×32 (downsample of `x4`)  \n",
        "   - `lr`: 16×16 (downsample of `x2`)  \n",
        "   All are converted to tensors with the same normalization as `hr`.\n",
        "\n",
        "4. **Heatmaps (optional)**  \n",
        "   If `heatmap_h5` is provided, the code loads **all** heatmaps into RAM from the `heatmaps` dataset inside the HDF5 file (shape `(N, 128, 128)`).  \n",
        "   - At `__getitem__`, it fetches `heatmaps[index]`, wraps it as a `(1,128,128)` tensor, and returns it.  \n",
        "   - If not provided, it returns a zero heatmap.  \n",
        "   **Tip:** ensure the HDF5 ordering matches CelebA’s file order used here (same split and sort).\n",
        "\n",
        "**Why this layout?**  \n",
        "- Produces supervision at **multiple scales** (16/32/64/128), which is standard for progressive face SR and aligns with setups that use **landmark‑based attention/heatmap losses** (as in Kim et al., 2019).  \n",
        "- Keeps the dataloader simple and fast: images are read from disk; heatmaps (small) can be preloaded to RAM.\n",
        "\n",
        "**Usage**\n",
        "```python\n",
        "ds = CelebDataSet(\n",
        "    data_path=\"/path/to/celeba\",\n",
        "    state=\"train\",\n",
        "    heatmap_h5=\"heatmaps_30k.h5\"  # or None\n",
        ")\n",
        "x2, x4, hr, lr, heat = ds[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU9cNqQklC0y"
      },
      "outputs": [],
      "source": [
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from os.path import join, splitext\n",
        "from PIL import Image\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class CelebDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    CelebA dataset with optional landmark-heatmap loading from HDF5.\n",
        "\n",
        "    Returns: (x2, x4, hr, lr, heatmap)\n",
        "      - x2: 32×32 target tensor\n",
        "      - x4: 64×64 target tensor\n",
        "      - hr: 128×128 target tensor\n",
        "      - lr: 16×16 input tensor\n",
        "      - heatmap: 1×128×128 float tensor\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str = './dataset/',\n",
        "        state: str = 'train',\n",
        "        data_augmentation: bool = False,\n",
        "        heatmap_h5: str = None,\n",
        "    ):\n",
        "        self.main_path = data_path\n",
        "        self.state = state\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.img_path = join(self.main_path, 'img_align_celeba/img_align_celeba/')\n",
        "        self.eval_partition_path = join(self.main_path, 'list_eval_partition.csv')\n",
        "\n",
        "        # load train/val/test split\n",
        "        train_list, val_list, test_list = [], [], []\n",
        "        with open(self.eval_partition_path, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for fname, split in reader:\n",
        "                fname, split = fname.strip(), split.strip()\n",
        "                if split == '0':\n",
        "                    train_list.append(fname)\n",
        "                elif split == '1':\n",
        "                    val_list.append(fname)\n",
        "                else:\n",
        "                    test_list.append(fname)\n",
        "\n",
        "        if state == 'train':\n",
        "            self.image_list = sorted(train_list)\n",
        "        elif state == 'val':\n",
        "            self.image_list = sorted(val_list)\n",
        "        else:\n",
        "            self.image_list = sorted(test_list)\n",
        "\n",
        "        # transforms\n",
        "        if state=='train' and data_augmentation:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "                transforms.RandomRotation(\n",
        "                    20,\n",
        "                    interpolation=InterpolationMode.BILINEAR\n",
        "                ),\n",
        "                transforms.ColorJitter(0.4,0.4,0.4,0.1)\n",
        "            ])\n",
        "        else:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "            ])\n",
        "\n",
        "        self.totensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "        ])\n",
        "        self.down64 = transforms.Resize((64,64))\n",
        "        self.down32 = transforms.Resize((32,32))\n",
        "        self.down16 = transforms.Resize((16,16))\n",
        "\n",
        "        # ACTUALLY load heatmaps into memory\n",
        "        if heatmap_h5:\n",
        "            with h5py.File(heatmap_h5, 'r') as h5_file:\n",
        "                # Load the entire heatmap dataset into RAM\n",
        "                self.heatmaps = np.array(h5_file['heatmaps'])  # Shape: (N, 128, 128)\n",
        "        else:\n",
        "            self.heatmaps = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image\n",
        "        fname = self.image_list[index]\n",
        "        img = Image.open(join(self.img_path, fname)).convert('RGB')\n",
        "        img = self.pre_process(img)\n",
        "\n",
        "        # build multi-scale\n",
        "        x4 = self.down64(img)    # 64x64\n",
        "        x2 = self.down32(x4)     # 32x32\n",
        "        lr = self.down16(x2)     # 16x16\n",
        "\n",
        "        # to tensor\n",
        "        hr_tensor = self.totensor(img)\n",
        "        x4_tensor = self.totensor(x4)\n",
        "        x2_tensor = self.totensor(x2)\n",
        "        lr_tensor = self.totensor(lr)\n",
        "\n",
        "        # load heatmap (already 128×128)\n",
        "        if self.heatmaps is not None:\n",
        "            hm = self.heatmaps[index]              # numpy array (128,128)\n",
        "            heat = torch.from_numpy(hm.copy()).unsqueeze(0)  # (1,128,128)\n",
        "        else:\n",
        "            heat = torch.zeros(1,128,128)\n",
        "\n",
        "        return x2_tensor, x4_tensor, hr_tensor, lr_tensor, heat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2xxIrdcc1ZX"
      },
      "source": [
        "### SuperResolutionUNet Architecture\n",
        "\n",
        "This is an **efficient U-Net–style model** for image super-resolution, inspired by encoder–decoder\n",
        "designs but optimized for speed and stability.  \n",
        "- **Input:** Low-resolution 16×16 RGB image.  \n",
        "- **Output:** High-resolution 128×128 RGB reconstruction.  \n",
        "- **Encoder:** Four `DoubleConv` blocks with strided convolutions instead of pooling, progressively downsampling features.  \n",
        "- **Bottleneck:** A deeper `DoubleConv` block processes the compressed representation.  \n",
        "- **Decoder:** Symmetric `UpBlock` modules (upsample + skip connection) restore spatial resolution to 16×16.  \n",
        "- **Learned Upsampling:** Three `UpLearn` stages (nearest-neighbor + conv) upscale from 16×16 → 32×32 → 64×64 → 128×128.  \n",
        "- **Refinement:** Several residual blocks refine details at full resolution.  \n",
        "- **Global Skip:** The bilinearly upscaled input is added to the output for stability and better identity preservation.  \n",
        "\n",
        "This structure is lightweight yet expressive, making it suitable for fast face SR experiments and attention-based enhancements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0i-twj4vWzD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def conv3x3(in_ch, out_ch, stride=1):\n",
        "    return nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=True)\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, stride=1):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            conv3x3(in_ch, out_ch, stride=stride),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            conv3x3(out_ch, out_ch),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    def __init__(self, in_ch, skip_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up   = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)\n",
        "        self.conv = DoubleConv(in_ch + skip_ch, out_ch)\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpLearn(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x): return self.up(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.body = nn.Sequential(\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ch, ch, 3, padding=1, bias=True),\n",
        "        )\n",
        "    def forward(self, x): return x + self.body(x)\n",
        "\n",
        "class SuperResolutionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Efficient U-Net SR (16x16 → 128x128)\n",
        "    - Strided convs instead of MaxPool\n",
        "    - align_corners=False\n",
        "    - 3x UpLearn (nearest+conv)\n",
        "    - Small refine head at 128x\n",
        "    - Optional heatmap injection at refine stage\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, base_filters=32, out_channels=3, refine_blocks=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Encoder (strided)\n",
        "        self.enc1 = DoubleConv(in_channels,       base_filters,   stride=1)  # 16x\n",
        "        self.enc2 = DoubleConv(base_filters,      base_filters*2, stride=2)  # 8x\n",
        "        self.enc3 = DoubleConv(base_filters*2,    base_filters*4, stride=2)  # 4x\n",
        "        self.enc4 = DoubleConv(base_filters*4,    base_filters*8, stride=2)  # 2x bottleneck in size after next\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = DoubleConv(base_filters*8, base_filters*8)\n",
        "\n",
        "        # Decoder (back to 16x)\n",
        "        self.up3 = UpBlock(base_filters*8, base_filters*4, base_filters*4)  # 2x->4x, skip e3\n",
        "        self.up2 = UpBlock(base_filters*4, base_filters*2, base_filters*2)  # 4x->8x, skip e2\n",
        "        self.up1 = UpBlock(base_filters*2, base_filters,   base_filters)    # 8x->16x, skip e1\n",
        "\n",
        "        # Learned upsampling to 128x\n",
        "        self.up_learn1 = UpLearn(base_filters)  # 16->32\n",
        "        self.up_learn2 = UpLearn(base_filters)  # 32->64\n",
        "        self.up_learn3 = UpLearn(base_filters)  # 64->128\n",
        "\n",
        "        # Refine head\n",
        "        refine_in = base_filters\n",
        "        self.refine_in = nn.Conv2d(refine_in, base_filters, 1)\n",
        "        self.refine = nn.Sequential(*[ResBlock(base_filters) for _ in range(refine_blocks)])\n",
        "\n",
        "        # Final projection + global skip\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        e1 = self.enc1(x)        # 16x\n",
        "        e2 = self.enc2(e1)       # 8x\n",
        "        e3 = self.enc3(e2)       # 4x\n",
        "        e4 = self.enc4(e3)       # 2x\n",
        "\n",
        "        b  = self.bottleneck(e4)\n",
        "\n",
        "        # decoder to 16x\n",
        "        d3 = self.up3(b, e3)     # 4x\n",
        "        d2 = self.up2(d3, e2)    # 8x\n",
        "        d1 = self.up1(d2, e1)    # 16x\n",
        "\n",
        "        # learned upsampling to 128x\n",
        "        u1 = self.up_learn1(d1)  # 32x\n",
        "        u2 = self.up_learn2(u1)  # 64x\n",
        "        u3 = self.up_learn3(u2)  # 128x\n",
        "\n",
        "        r = self.refine(self.refine_in(u3))\n",
        "        out = self.final_conv(r)\n",
        "\n",
        "        # global residual skip (bilinear, no corners)\n",
        "        up_input = F.interpolate(x, size=out.shape[2:], mode='bilinear', align_corners=False)\n",
        "        return out + up_input\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxg4fxDRc9x2"
      },
      "source": [
        "### Loss Functions for Super-Resolution Training\n",
        "\n",
        "This section defines multiple complementary loss functions used to guide the SR model:\n",
        "\n",
        "1. **Pixel Loss (MSE)**  \n",
        "   - Standard mean squared error between the super-resolved (`sr`) and high-resolution (`hr`) images.  \n",
        "   - Encourages overall pixel-level accuracy.\n",
        "\n",
        "2. **Perceptual Loss (VGG-based)**  \n",
        "   - Extracts features from a pretrained VGG-16 network (layers conv1_2, conv2_2, conv3_3).  \n",
        "   - Inputs are expected in `[-1, 1]` range; internally mapped to `[0, 1]` and normalized with ImageNet statistics.  \n",
        "   - Computes the sum of MSE losses between corresponding SR and HR feature maps.  \n",
        "   - Focuses on perceptual similarity and high-level structure rather than exact pixel match.\n",
        "\n",
        "3. **Attention Loss (Masked MAE)**  \n",
        "   - Uses a heatmap (e.g., from facial landmark detection) to weight pixel errors, focusing more on important regions (eyes, mouth, nose, etc.).  \n",
        "   - Supports parameters:\n",
        "     - `gamma`: Controls how strongly to emphasize hot areas.\n",
        "     - `floor`: Minimum weight for pixels outside the mask to keep gradients stable.\n",
        "   - Normalizes weights per sample so the loss is scale-invariant.\n",
        "\n",
        "4. **LPIPS Loss**  \n",
        "   - Learned perceptual similarity metric based on deep features (VGG backbone).  \n",
        "   - Inputs in `[-1, 1]`; returns a scalar measuring perceptual distance between SR and HR.  \n",
        "   - Complements MSE/VGG losses by capturing human-judged similarity.\n",
        "\n",
        "**Usage:**  \n",
        "These losses are typically combined with tuned weights (e.g., more weight on perceptual/attention losses for better visual quality, more on pixel loss for higher PSNR).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4zij_PllY40"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "import lpips\n",
        "\n",
        "# -------------------------\n",
        "# Pixel loss\n",
        "# -------------------------\n",
        "pixel_crit = nn.MSELoss()\n",
        "\n",
        "# -------------------------\n",
        "# Perceptual (VGG) loss\n",
        "# -------------------------\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Expects inputs in [-1,1]. Internally maps to [0,1] and applies ImageNet mean/std.\n",
        "    Runs VGG feature extraction in float32 (even under autocast) for stability.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(weights=VGG16_Weights.IMAGENET1K_FEATURES).features\n",
        "        self.slice1 = nn.Sequential(*list(vgg[:4])).eval()   # conv1_2\n",
        "        self.slice2 = nn.Sequential(*list(vgg[4:9])).eval()  # conv2_2\n",
        "        self.slice3 = nn.Sequential(*list(vgg[9:16])).eval() # conv3_3\n",
        "        for m in (self.slice1, self.slice2, self.slice3):\n",
        "            for p in m.parameters():\n",
        "                p.requires_grad = False\n",
        "\n",
        "        # ImageNet norm buffers\n",
        "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
        "        self.register_buffer('std',  torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
        "\n",
        "    def _prep(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # [-1,1] -> [0,1] -> ImageNet norm\n",
        "        x01 = (x.clamp(-1,1) + 1) / 2\n",
        "        return (x01 - self.mean) / self.std\n",
        "\n",
        "    def forward(self, sr: torch.Tensor, hr: torch.Tensor) -> torch.Tensor:\n",
        "        # Force fp32 for VGG path even if outer training is mixed precision\n",
        "        sr32 = self._prep(sr).float()\n",
        "        hr32 = self._prep(hr).float()\n",
        "\n",
        "        f1_sr, f1_hr = self.slice1(sr32), self.slice1(hr32)\n",
        "        f2_sr, f2_hr = self.slice2(f1_sr),  self.slice2(f1_hr)\n",
        "        f3_sr, f3_hr = self.slice3(f2_sr),  self.slice3(f2_hr)\n",
        "\n",
        "        # Sum of MSEs across a few layers\n",
        "        return (F.mse_loss(f1_sr, f1_hr) +\n",
        "                F.mse_loss(f2_sr, f2_hr) +\n",
        "                F.mse_loss(f3_sr, f3_hr))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "perceptual_crit = VGGPerceptualLoss().to(device)\n",
        "\n",
        "def attention_loss(\n",
        "    sr, hr, heat,\n",
        "    *, gamma: float = 1.3,   # >1 = more focus on hot zones\n",
        "       floor: float = 0.10,  # weight outside the mask (0..1)\n",
        "       eps: float = 1e-6\n",
        "):\n",
        "    \"\"\"\n",
        "    Per-sample normalized masked MAE:\n",
        "      loss_i = sum(w_i * |sr - hr|) / sum(w_i), then averaged over the batch.\n",
        "    - sr, hr: (B, C, H, W) in [-1, 1]\n",
        "    - heat: (B, 1, H, W) or (B, H, W)\n",
        "    - gamma: selectivity of the heatmap\n",
        "    - floor: minimum gradient even outside the mask\n",
        "    \"\"\"\n",
        "    if heat is None:\n",
        "        return sr.new_tensor(0.0)\n",
        "\n",
        "    if heat.dim() == 3:\n",
        "        heat = heat.unsqueeze(1)  # (B,1,H,W)\n",
        "\n",
        "    heat = heat.to(device=sr.device, dtype=sr.dtype)\n",
        "    B = heat.size(0)\n",
        "\n",
        "    # min-max per campione -> [0,1]\n",
        "    flat = heat.reshape(B, -1)\n",
        "    hmin = flat.min(dim=1, keepdim=True)[0].reshape(B,1,1,1)\n",
        "    hmax = flat.max(dim=1, keepdim=True)[0].reshape(B,1,1,1)\n",
        "    span = (hmax - hmin)\n",
        "\n",
        "    hn = (heat - hmin) / span.clamp_min(eps)     # [0,1]\n",
        "    if abs(gamma - 1.0) > 1e-6:\n",
        "        hn = hn.clamp(0,1).pow(gamma)\n",
        "\n",
        "    # w' = floor + (1-floor)*hn  in [floor,1]\n",
        "    w = floor + (1.0 - floor) * hn\n",
        "\n",
        "    # se mappa ~costante, usa pesi uniformi (tutti 1)\n",
        "    uniform = (span <= eps)\n",
        "    if uniform.any():\n",
        "        w = torch.where(uniform, torch.ones_like(w), w)\n",
        "\n",
        "    # niente grad attraverso i pesi\n",
        "    w = w.expand_as(sr).detach()\n",
        "\n",
        "    # riduzione per-sample (reshape evita problemi di contiguità)\n",
        "    w_flat   = w.reshape(B, -1)\n",
        "    mae_flat = (w * (sr - hr).abs()).reshape(B, -1)\n",
        "    loss_per_sample = mae_flat.sum(dim=1) / w_flat.sum(dim=1).clamp_min(eps)\n",
        "    return loss_per_sample.mean()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# LPIPS loss\n",
        "# -------------------------\n",
        "# lpips expects inputs in [-1,1]; returns (B,1,1,1) or (B,)\n",
        "lpips_crit = lpips.LPIPS(net='vgg').to(device)\n",
        "\n",
        "def lpips_loss(sr, hr):\n",
        "    return lpips_crit(sr, hr).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOa-6TP5eWPC"
      },
      "source": [
        "## `Trainer` Class Overview\n",
        "\n",
        "The `Trainer` class handles **training**, **evaluation**, and **metric computation** for a **Super-Resolution U-Net** model with optional perceptual, attention, and LPIPS losses.\n",
        "It supports **mixed-precision training** with `GradScaler` and computes multiple image quality metrics during training.\n",
        "\n",
        "---\n",
        "\n",
        "### **Initialization (`__init__`)**\n",
        "\n",
        "* **Stores configuration** (`cfg`).\n",
        "* Builds the **Generator network** (`SuperResolutionUNet`) with parameters from `cfg`.\n",
        "* Creates an **Adam optimizer** for the generator.\n",
        "* Sets up a **`GradScaler`** for mixed-precision training (enabled if CUDA is available).\n",
        "* Defines `metric_stride` to control how often metrics are computed during training.\n",
        "* Initializes **Multi-Scale SSIM** metric object (`MSSSIMMetric`) configured for images in `[-1, 1]`.\n",
        "\n",
        "---\n",
        "\n",
        "### **`psnr_from_mse(mse, max_val)`**\n",
        "\n",
        "* Computes **Peak Signal-to-Noise Ratio (PSNR)** from a Mean Squared Error value, assuming a given pixel value range (`max_val=2.0` for `[-1, 1]`).\n",
        "\n",
        "---\n",
        "\n",
        "### **`compute_overall_metrics(sr, hr)`**\n",
        "\n",
        "* Takes **super-resolved** (`sr`) and **high-resolution** (`hr`) tensors in `[-1, 1]`.\n",
        "* Computes:\n",
        "\n",
        "  * **PSNR** (per batch, averaged).\n",
        "  * **SSIM** (single-scale).\n",
        "  * **MS-SSIM** (multi-scale).\n",
        "  * **LPIPS** (perceptual similarity).\n",
        "* Returns all metrics in a dictionary.\n",
        "\n",
        "---\n",
        "\n",
        "### **`train_epoch(loader)`**\n",
        "\n",
        "* Runs one full training epoch over the data `loader`.\n",
        "* Steps:\n",
        "\n",
        "  1. Sets model to **train** mode.\n",
        "  2. Iterates over the dataloader:\n",
        "\n",
        "     * Moves data to GPU.\n",
        "     * **Forward pass** inside `autocast` for mixed precision:\n",
        "\n",
        "       * Computes pixel loss (**MSE**).\n",
        "       * Optionally computes **perceptual loss**, **attention loss**, **LPIPS loss** depending on `cfg`.\n",
        "       * Combines them using configured weights (`w_perc`, `w_attn`, `w_lpips`).\n",
        "     * **Backward pass** using gradient scaling.\n",
        "     * Updates optimizer.\n",
        "  3. Every `metric_stride` steps, computes and accumulates **overall quality metrics**.\n",
        "  4. Returns **average losses and metrics** over the epoch.\n",
        "\n",
        "---\n",
        "\n",
        "### **`evaluate(loader, num_samples)`**\n",
        "\n",
        "* Runs evaluation (no gradients) on at most `num_samples` batches.\n",
        "* Steps:\n",
        "\n",
        "  1. Sets model to **eval** mode.\n",
        "  2. Iterates over the loader:\n",
        "\n",
        "     * Computes `sr` outputs.\n",
        "     * Measures overall metrics (`psnr_overall`, `ssim_overall`, `msssim_overall`, `lpips_overall`).\n",
        "     * Computes all active loss components and the combined loss.\n",
        "  3. Returns **average losses and metrics** over evaluated samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0-5vDrLhyKh"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import autocast, GradScaler\n",
        "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "from torchmetrics.image.ssim import MultiScaleStructuralSimilarityIndexMeasure as MSSSIMMetric\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg    = cfg\n",
        "        self.G = SuperResolutionUNet(\n",
        "            in_channels   = cfg.get('in_channels', 3),\n",
        "            base_filters  = cfg.get('base_filters', 32),\n",
        "            out_channels  = cfg.get('out_channels', 3),\n",
        "            refine_blocks = cfg.get('refine_blocks', 3)\n",
        "        ).to(device)\n",
        "        self.optG   = torch.optim.Adam(self.G.parameters(), lr=cfg['lr_g'])\n",
        "        self.scaler = GradScaler(enabled=torch.cuda.is_available())\n",
        "        self.metric_stride = 5  # compute metrics every N batches\n",
        "        self.msssim_metric = MSSSIMMetric(\n",
        "            data_range=2.0,\n",
        "            kernel_size=(7, 7),                         # smaller window for 128×128\n",
        "            betas=(0.0448, 0.2856, 0.3001, 0.2363),     # 4 scales\n",
        "            normalize=\"relu\",\n",
        "        ).to(device)\n",
        "\n",
        "    @staticmethod\n",
        "    def psnr_from_mse(mse, max_val=2.0):  # max_val=2.0 for [-1,1]\n",
        "        return 10.0 * torch.log10((max_val * max_val) / (mse + 1e-8))\n",
        "\n",
        "    def compute_overall_metrics(self, sr, hr):\n",
        "        \"\"\"Overall PSNR/SSIM/MS-SSIM/LPIPS with inputs in [-1,1].\"\"\"\n",
        "        with torch.no_grad():\n",
        "            mse_overall = ((sr - hr) ** 2).mean(dim=(1,2,3))\n",
        "            psnr_overall   = self.psnr_from_mse(mse_overall, max_val=2.0).mean().item()\n",
        "            ssim_overall   = float(ssim(sr, hr, data_range=2.0))\n",
        "            msssim_overall = float(self.msssim_metric(sr, hr).cpu())\n",
        "            self.msssim_metric.reset()\n",
        "            lpips_overall  = lpips_loss(sr, hr)\n",
        "            if torch.is_tensor(lpips_overall):\n",
        "                lpips_overall = lpips_overall.mean().item()\n",
        "            return {\n",
        "                'psnr_overall': psnr_overall,\n",
        "                'ssim_overall': ssim_overall,\n",
        "                'msssim_overall': msssim_overall,\n",
        "                'lpips_overall': lpips_overall\n",
        "            }\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        agg = {\n",
        "            'loss_pixel': 0.0, 'loss_perc': 0.0, 'loss_attn': 0.0,\n",
        "            'loss_lpips': 0.0, 'loss_combined': 0.0,\n",
        "            'psnr_overall': 0.0, 'ssim_overall': 0.0,\n",
        "            'msssim_overall': 0.0, 'lpips_overall': 0.0\n",
        "        }\n",
        "        self.G.train()\n",
        "        step = 0\n",
        "        metric_steps = 0\n",
        "        use_cuda = torch.cuda.is_available()\n",
        "\n",
        "        for _, _, hr, lr, heat in tqdm(loader, desc=f\"Training {self.cfg['name']}\"):\n",
        "            lr   = lr.to(device, non_blocking=True)\n",
        "            hr   = hr.to(device, non_blocking=True)\n",
        "            heat = heat.to(device, non_blocking=True) if self.cfg.get('use_attn', False) else None\n",
        "\n",
        "            self.optG.zero_grad(set_to_none=True)\n",
        "            with autocast(device_type='cuda', enabled=use_cuda, dtype=torch.float16):\n",
        "                sr    = self.G(lr)\n",
        "                Lpix  = pixel_crit(sr, hr)\n",
        "                Lperc = perceptual_crit(sr, hr)              if self.cfg.get('use_perc',  False) else 0.0\n",
        "                Lattn = attention_loss(sr, hr, heat)         if self.cfg.get('use_attn',  False) else 0.0\n",
        "                Llp   = lpips_loss(sr, hr)                   if self.cfg.get('use_lpips', False) else 0.0\n",
        "\n",
        "                loss  = Lpix \\\n",
        "                      + self.cfg.get('w_perc',0.0)  * (Lperc if isinstance(Lperc, torch.Tensor) else 0.0) \\\n",
        "                      + self.cfg.get('w_attn',0.0)  * (Lattn if isinstance(Lattn, torch.Tensor) else 0.0) \\\n",
        "                      + self.cfg.get('w_lpips',0.0) * (Llp   if isinstance(Llp,   torch.Tensor) else 0.0)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optG)\n",
        "            self.scaler.update()\n",
        "\n",
        "            agg['loss_pixel']    += float(Lpix.detach())\n",
        "            agg['loss_perc']     += float(Lperc.detach()) if self.cfg.get('use_perc',  False) else 0.0\n",
        "            agg['loss_attn']     += float(Lattn.detach()) if self.cfg.get('use_attn',  False) else 0.0\n",
        "            agg['loss_lpips']    += float(Llp.detach())   if self.cfg.get('use_lpips', False) else 0.0\n",
        "            agg['loss_combined'] += float(loss.detach())\n",
        "\n",
        "            if step % self.metric_stride == 0:\n",
        "                m = self.compute_overall_metrics(sr.detach(), hr)\n",
        "                for k in ('psnr_overall','ssim_overall','msssim_overall','lpips_overall'):\n",
        "                    agg[k] += m[k]\n",
        "                metric_steps += 1\n",
        "\n",
        "            step += 1\n",
        "\n",
        "        out = {}\n",
        "        for k in ('loss_pixel','loss_perc','loss_attn','loss_lpips','loss_combined'):\n",
        "            out[k] = agg[k] / step\n",
        "        for k in ('psnr_overall','ssim_overall','msssim_overall','lpips_overall'):\n",
        "            out[k] = agg[k] / (metric_steps if metric_steps > 0 else 1)\n",
        "        return out\n",
        "\n",
        "    def evaluate(self, loader, num_samples=500):\n",
        "        self.G.eval()\n",
        "        agg = {\n",
        "            'loss_pixel':0.0, 'loss_perc':0.0, 'loss_attn':0.0, 'loss_lpips':0.0, 'loss_combined':0.0,\n",
        "            'psnr_overall':0.0, 'ssim_overall':0.0, 'msssim_overall':0.0, 'lpips_overall':0.0\n",
        "        }\n",
        "        n = 0\n",
        "        with torch.no_grad():\n",
        "            for _, _, hr, lr, heat in tqdm(loader, desc=f\"Evaluating {self.cfg['name']}\"):\n",
        "                if n >= num_samples:\n",
        "                    break\n",
        "                lr   = lr.to(device, non_blocking=True)\n",
        "                hr   = hr.to(device, non_blocking=True)\n",
        "                heat = heat.to(device, non_blocking=True) if self.cfg.get('use_attn', False) else None\n",
        "\n",
        "                sr = self.G(lr)\n",
        "\n",
        "                m = self.compute_overall_metrics(sr, hr)\n",
        "                for k in ('psnr_overall','ssim_overall','msssim_overall','lpips_overall'):\n",
        "                    agg[k] += m[k]\n",
        "\n",
        "                Lpix  = pixel_crit(sr, hr)\n",
        "                Lperc = perceptual_crit(sr, hr)              if self.cfg.get('use_perc',  False) else 0.0\n",
        "                Lattn = attention_loss(sr, hr, heat)         if self.cfg.get('use_attn',  False) else 0.0\n",
        "                Llp   = lpips_loss(sr, hr)                   if self.cfg.get('use_lpips', False) else 0.0\n",
        "                Lcomb = Lpix \\\n",
        "                      + self.cfg.get('w_perc',0.0)  * (Lperc if isinstance(Lperc, torch.Tensor) else 0.0) \\\n",
        "                      + self.cfg.get('w_attn',0.0)  * (Lattn if isinstance(Lattn, torch.Tensor) else 0.0) \\\n",
        "                      + self.cfg.get('w_lpips',0.0) * (Llp   if isinstance(Llp,   torch.Tensor) else 0.0)\n",
        "\n",
        "                agg['loss_pixel']    += float(Lpix)\n",
        "                agg['loss_perc']     += float(Lperc) if self.cfg.get('use_perc', False) else 0.0\n",
        "                agg['loss_attn']     += float(Lattn) if self.cfg.get('use_attn', False) else 0.0\n",
        "                agg['loss_lpips']    += float(Llp)   if self.cfg.get('use_lpips', False) else 0.0\n",
        "                agg['loss_combined'] += float(Lcomb)\n",
        "\n",
        "                n += 1\n",
        "\n",
        "        return {k: (v / max(n,1)) for k, v in agg.items()}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLhuW56Iewgh"
      },
      "source": [
        "# Training Script Overview\n",
        "\n",
        "This script wires up datasets, a `Trainer`, schedulers, and utilities to **train/evaluate** a Super-Resolution U-Net with optional losses (pixel/perceptual/LPIPS/attention), prints metrics (incl. **MS-SSIM**), **adapts loss weights over time**, and handles **checkpointing + early stop**.\n",
        "\n",
        "---\n",
        "\n",
        "## 1) Setup & Reproducibility\n",
        "\n",
        "* Selects `device` (CUDA if available) and fixes seeds (`torch`, `numpy`) with deterministic cuDNN.\n",
        "* Optional pretrained checkpoint path: `ckpt_path`.\n",
        "\n",
        "---\n",
        "\n",
        "## 2) Data, Limits, and Loaders\n",
        "\n",
        "* Reads the number of available heatmaps from `heatmaps.h5` → `HM_COUNT`.\n",
        "* Sets **training limit** to `HM_COUNT` and **validation limit** to 20% of that.\n",
        "* Builds `CelebDataSet` for `'train'` and `'val'`, clamps subset sizes to the limits, and wraps them in `DataLoader`s (batch size 64, 2 workers, `pin_memory` if CUDA).\n",
        "* Prepares a **fixed sample** (`lr_vis`, `hr_vis`, `heat_vis`) from the first training batch for quick visualization.\n",
        "\n",
        "---\n",
        "\n",
        "## 3) Model Config(s) → Trainers\n",
        "\n",
        "* Defines one config (ablation example):\n",
        "\n",
        "  * `name='ABLATION_30K_mse+vgg+lpips'`\n",
        "  * `use_attn=False`, `use_perc=True`, `use_lpips=True`\n",
        "  * loss weights initialized: `w_attn=0`, `w_perc=0.02`, `w_lpips=0.15`\n",
        "  * optimizer LR `lr_g=1e-4`\n",
        "  * UNet capacity: `base_filters=48`, `refine_blocks=5`\n",
        "* For each config:\n",
        "\n",
        "  * Instantiates a `Trainer(cfg)` (your Trainer class builds the SR-UNet, optimizer, AMP scaler, metrics).\n",
        "  * Attaches **ReduceLROnPlateau** (`mode='min'`, `patience=6`, `factor=0.5`) targeting a **validation objective** (see §6).\n",
        "\n",
        "---\n",
        "\n",
        "## 4) Utilities\n",
        "\n",
        "### LossWeightScheduler\n",
        "\n",
        "Time-based weights over the course of training (`epoch/num_epochs`):\n",
        "\n",
        "* **0–30% (warm-up):** ramp `w_perc` 0→0.06, `w_attn` 0.7→1.2, `w_lpips` off.\n",
        "* **30–85% (mid):** `w_perc` 0.06→0.09, `w_attn` 1.2→2.0, `w_lpips` 0.00→0.22.\n",
        "* **85–100% (late):** `w_perc` fixed 0.09, `w_attn` 2.0→1.0, `w_lpips` 0.22→0.25.\n",
        "  (If a loss is disabled in `cfg`, its weight is forced to 0.)\n",
        "\n",
        "### EarlyStopping\n",
        "\n",
        "* Tracks best **validation objective**; stops if it hasn’t improved by `min_delta=1e-4` for `patience=15` epochs.\n",
        "\n",
        "### Validation Objective (`val_objective`)\n",
        "\n",
        "Scalar to **minimize** (lower is better):\n",
        "\n",
        "```\n",
        "1.5*LPIPS + 0.8*max(0, 0.02 - SSIM) + 0.2*max(0, 20 - PSNR)\n",
        "```\n",
        "\n",
        "Penalizes high LPIPS, SSIM below 0.02, and PSNR below 20 dB.\n",
        "\n",
        "### Misc\n",
        "\n",
        "* `to01(x)`: maps tensors from `[-1,1]` → `[0,1]` for visualization.\n",
        "* `save_checkpoint(...)`: saves a **full** training state dict (model/optimizer/scaler/scheduler + early-best).\n",
        "\n",
        "---\n",
        "\n",
        "## 5) (Optional) Checkpoint Restore\n",
        "\n",
        "If `ckpt_path` is set:\n",
        "\n",
        "* Loads `model/opt/scaler/sched` states into the trainer.\n",
        "* Restores `early_stop.best` (best validation objective so far).\n",
        "* Sets `start_epoch = saved_epoch + 1`.\n",
        "  Prints a confirmation.\n",
        "\n",
        "---\n",
        "\n",
        "## 6) Epoch Loop\n",
        "\n",
        "For `epoch in start_epoch .. num_epochs`:\n",
        "\n",
        "### a) Update Loss Weights\n",
        "\n",
        "* Calls `LossWeightScheduler.weights_at(...)` and writes `w_perc/w_attn/w_lpips` into each `cfg`.\n",
        "\n",
        "### b) TRAIN\n",
        "\n",
        "* For each config:\n",
        "\n",
        "  * `train_epoch(loader)` on its `Trainer`:\n",
        "\n",
        "    * Mixed precision + gradient scaling.\n",
        "    * Computes the enabled losses and their **weighted sum**.\n",
        "    * Periodically (every `metric_stride` steps from the `Trainer`) accumulates **PSNR / SSIM / MS-SSIM / LPIPS** on the training stream.\n",
        "  * Appends per-epoch training metrics to `history`.\n",
        "\n",
        "* Prints a **training table** per model with:\n",
        "\n",
        "  * `pix`, `perc`, `attn`, `lpips_loss`, `comb`\n",
        "  * `PSNR`, `SSIM`, `MS-SSIM`, `LPIPS`\n",
        "\n",
        "### c) VALIDATION\n",
        "\n",
        "* For each config:\n",
        "\n",
        "  * `evaluate(val_loader)` (no grad, averages over the set).\n",
        "  * Appends validation metrics to `history` (prefixed with `val_`).\n",
        "\n",
        "* Computes the **reference** validation objective on the **first config** (`ref_name = configs[0]['name']`), then:\n",
        "\n",
        "  * Steps each **ReduceLROnPlateau** with that model’s own validation objective.\n",
        "  * Steps **EarlyStopping** with the reference objective.\n",
        "\n",
        "* Prints a **validation table** with the same metrics plus `val_obj`.\n",
        "\n",
        "### d) Quick Visualization\n",
        "\n",
        "* Grabs 5 examples from the validation loader.\n",
        "* Shows: **upsampled LR**, **HR GT**, and **each model’s reconstruction** (mapped to `[0,1]`).\n",
        "\n",
        "### e) Save Best & Periodic Checkpoints\n",
        "\n",
        "* **Best (by val objective, first config):** saves `./<ref_name>/<ref_name>_best.pth` (weights only).\n",
        "* **Every 10 epochs:** saves a **full checkpoint** (`.pt`) for each config via `save_checkpoint(...)` and triggers `files.download(path)` (handy in Colab).\n",
        "\n",
        "### f) Early Stop\n",
        "\n",
        "* If `early_stop.should_stop` is set, prints a message with the **best val objective** and breaks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fR8am-MfnEYK"
      },
      "outputs": [],
      "source": [
        "# --- core imports\n",
        "import os, h5py, numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.functional as F\n",
        "import h5py\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "# --- device & seeds\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "pin = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "np.random.seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# --- paths & basic hyperparams\n",
        "data_path  = path            # <- set this to your CelebA root\n",
        "heat_h5    = './heatmaps.h5' # <- your heatmaps file\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "ckpt_path = None\n",
        "# Uncomment this and the other part in the code if loading a pretrained version of the model\n",
        "#ckpt_path = \"./PATH_TO_CHECKPOINT.pt\"\n",
        "\n",
        "with h5py.File(heat_h5, 'r') as f:\n",
        "    HM_COUNT = int(f['heatmaps'].shape[0])\n",
        "print(\"Heatmaps available:\", HM_COUNT)\n",
        "\n",
        "TRAIN_LIMIT = HM_COUNT                        # train limited by heatmaps (check not needed but assure consistency and prevents errors)\n",
        "VAL_LIMIT   = max(1, int(0.2 * TRAIN_LIMIT))  # 20% of train (if we have 10k training samples, we limit our validation to be 2000)\n",
        "\n",
        "# --- build datasets\n",
        "train_ds = CelebDataSet(data_path, 'train', heatmap_h5=heat_h5)\n",
        "val_ds   = CelebDataSet(data_path, 'val',   heatmap_h5=heat_h5)\n",
        "\n",
        "train_n = min(TRAIN_LIMIT, len(train_ds))\n",
        "val_n   = min(VAL_LIMIT,   len(val_ds))\n",
        "\n",
        "print(f\"Using TRAIN_LIMIT={train_n}, VAL_LIMIT={val_n}\")\n",
        "\n",
        "train_subset = Subset(train_ds, range(train_n))\n",
        "val_subset   = Subset(val_ds,   range(val_n))\n",
        "\n",
        "# --- dataloaders\n",
        "loader     = DataLoader(train_subset, batch_size=batch_size, shuffle=True,\n",
        "                        num_workers=2, pin_memory=pin)\n",
        "val_loader = DataLoader(val_subset,   batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=2, pin_memory=pin)\n",
        "\n",
        "print(\"Train samples:\", len(train_subset))\n",
        "print(\"Val samples:  \", len(val_subset))\n",
        "\n",
        "# --- one fixed batch for viz\n",
        "# This batch will be used to visualize validation results during validation process in the training loop\n",
        "data_iter = iter(loader)\n",
        "_, _, hr_f, lr_f, heat_f = next(data_iter)\n",
        "lr_vis, hr_vis, heat_vis = [t.to(device, non_blocking=True) for t in (lr_f[0:1], hr_f[0:1], heat_f[0:1])]\n",
        "\n",
        "# --- single config (you can add more later)\n",
        "configs = [\n",
        "    # This commented is the model that worked better. Take it as reference for changes.\n",
        "    dict(\n",
        "        name='base_on_10k',\n",
        "        use_attn=True, use_perc=True, use_lpips=True,\n",
        "        w_attn=2.0, w_perc=0.02, w_lpips=0.15,\n",
        "        lr_g=1e-4,\n",
        "        base_filters=48,\n",
        "        refine_blocks=5\n",
        "    ),\n",
        "]\n",
        "\n",
        "for cfg in configs:\n",
        "    for k in ('w_perc','w_attn','w_lpips'):\n",
        "        cfg.setdefault(k, 0.0)\n",
        "        cfg.setdefault(f'_base_{k}', cfg[k])\n",
        "    cfg.setdefault('use_perc', False)\n",
        "    cfg.setdefault('use_attn', False)\n",
        "    cfg.setdefault('use_lpips', False)\n",
        "    cfg.setdefault('schedule', True)\n",
        "\n",
        "# --- build trainers from configs\n",
        "trainers = {}\n",
        "for cfg in configs:\n",
        "    t = Trainer(cfg)\n",
        "    if 'metric_stride' in cfg:\n",
        "        t.metric_stride = cfg['metric_stride']\n",
        "    t.lr_sched = ReduceLROnPlateau(t.optG, mode='min', patience=6, factor=0.5, verbose=True)\n",
        "    trainers[cfg['name']] = t\n",
        "\n",
        "# ==== helpers ===============================================================\n",
        "# Dynamically adjusts the weights of perceptual, attention, and LPIPS losses\n",
        "# throughout training based on the current epoch fraction.\n",
        "#\n",
        "# The schedule is divided into three phases:\n",
        "#   1. Warm-up (0–30% of total epochs): Gradually increases perceptual and attention\n",
        "#      loss weights from their starting values; LPIPS loss remains off.\n",
        "#   2. Mid-phase (30–85%): Slowly ramps perceptual and LPIPS weights to target values,\n",
        "#      and increases attention weight further.\n",
        "#   3. Late-phase (85–100%): Keeps perceptual weight fixed, slightly decreases attention\n",
        "#      weight, and slightly boosts LPIPS weight to emphasize fine texture details.\n",
        "#\n",
        "# If a given loss type is disabled in `cfg`, its weight is kept at 0 for the whole schedule.\n",
        "# 2) Scheduler che produce SCALE, poi moltiplica per i target base\n",
        "class LossWeightScheduler:\n",
        "    def __init__(self, num_epochs):\n",
        "        self.E = num_epochs\n",
        "\n",
        "    def scales_at(self, epoch, cfg):\n",
        "        \"\"\"Ritorna scale (0..1) per perc/attn/lpips, NON pesi assoluti.\"\"\"\n",
        "        t = epoch / self.E  # 0..1\n",
        "\n",
        "        if t <= 0.30:\n",
        "            u = (t / 0.30)\n",
        "            s_perc  = 0.00 + 1.00 * u      # 0 -> 1.00 del target base\n",
        "            s_attn  = 0.60 + 0.40 * u      # 0.60 -> 1.00 del target base\n",
        "            s_lpips = 0.00                 # off\n",
        "        elif t <= 0.85:\n",
        "            u = (t - 0.30) / 0.55\n",
        "            s_perc  = 1.00                 # già pieno target\n",
        "            s_attn  = 1.00                 # già pieno target\n",
        "            s_lpips = 0.00 + 1.00 * u      # 0 -> 1.00 del target base\n",
        "        else:\n",
        "            u = (t - 0.85) / 0.15\n",
        "            s_perc  = 1.00\n",
        "            s_attn  = 1.00 - 0.25 * u      # cala al 75% del target base\n",
        "            s_lpips = 1.00                 # pieno target\n",
        "        return s_perc, s_attn, s_lpips\n",
        "\n",
        "    def apply(self, epoch, cfg):\n",
        "        if not cfg.get('schedule', True):\n",
        "            # non toccare nulla\n",
        "            return\n",
        "        use_perc  = cfg.get('use_perc',  False)\n",
        "        use_attn  = cfg.get('use_attn',  False)\n",
        "        use_lpips = cfg.get('use_lpips', False)\n",
        "\n",
        "        s_perc, s_attn, s_lpips = self.scales_at(epoch, cfg)\n",
        "\n",
        "        base_perc  = cfg.get('_base_w_perc',  0.0)\n",
        "        base_attn  = cfg.get('_base_w_attn',  0.0)\n",
        "        base_lpips = cfg.get('_base_w_lpips', 0.0)\n",
        "\n",
        "        cfg['w_perc']  = (base_perc  * s_perc)  if use_perc  else 0.0\n",
        "        cfg['w_attn']  = (base_attn  * s_attn)  if use_attn  else 0.0\n",
        "        cfg['w_lpips'] = (base_lpips * s_lpips) if use_lpips else 0.0\n",
        "\n",
        "\n",
        "# Monitors a validation score and triggers early stopping when it stops improving.\n",
        "#\n",
        "# Parameters:\n",
        "#   patience  – number of consecutive epochs without significant improvement\n",
        "#               (greater than `min_delta`) before stopping.\n",
        "#   min_delta – minimum required improvement in the monitored score to be considered progress.\n",
        "#\n",
        "# Behavior:\n",
        "#   - Tracks the best (lowest) score seen so far.\n",
        "#   - Resets the bad epoch counter when improvement is detected.\n",
        "#   - Increments the bad epoch counter otherwise.\n",
        "#   - Sets `should_stop=True` when the bad epoch counter reaches `patience`.\n",
        "\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=15, min_delta=1e-4):\n",
        "        self.patience=patience; self.min_delta=min_delta\n",
        "        self.best=None; self.bad_epochs=0; self.should_stop=False\n",
        "    def step(self, score):\n",
        "        if self.best is None or score < self.best - self.min_delta:\n",
        "            self.best=score; self.bad_epochs=0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            if self.bad_epochs >= self.patience: self.should_stop=True\n",
        "\n",
        "# Computes a scalar \"validation objective\" score to guide model selection,\n",
        "# learning rate scheduling, and early stopping. Lower is better.\n",
        "#\n",
        "# The score is a weighted sum of penalties:\n",
        "#   - 1.5 * LPIPS: strong penalty for poor perceptual similarity (higher LPIPS).\n",
        "#   - 0.8 * max(0, 0.02 - SSIM): penalty if SSIM falls below 0.02 (no penalty otherwise).\n",
        "#   - 0.2 * max(0, 20.0 - PSNR): penalty if PSNR is below 20 dB (no penalty otherwise).\n",
        "#\n",
        "# Inputs:\n",
        "#   m – dictionary of validation metrics containing:\n",
        "#       'ssim_overall', 'lpips_overall', 'psnr_overall'\n",
        "#\n",
        "# Output:\n",
        "#   A single float representing the validation objective; smaller values indicate better quality.\n",
        "def val_objective(m):\n",
        "    ssim = float(m.get('ssim_overall', 0.0))\n",
        "    lp   = float(m.get('lpips_overall', 1.0))\n",
        "    psnr = float(m.get('psnr_overall', 0.0))\n",
        "    return (1.5*lp) + (0.8*max(0.0, 0.02-ssim)) + (0.2*max(0.0, 20.0-psnr))\n",
        "\n",
        "def to01(x): return (x.clamp(-1,1) + 1)/2\n",
        "\n",
        "def save_checkpoint(trainer, history, epoch, best_score, path):\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model': trainer.G.state_dict(),\n",
        "        'opt': trainer.optG.state_dict(),\n",
        "        'scaler': trainer.scaler.state_dict(),\n",
        "        'sched': trainer.lr_sched.state_dict() if hasattr(trainer, 'lr_sched') else None,\n",
        "        'early_best': best_score,\n",
        "        'history': history,\n",
        "    }, path)\n",
        "\n",
        "# ===========================================================================\n",
        "\n",
        "\n",
        "loss_sched = LossWeightScheduler(num_epochs)\n",
        "early_stop = EarlyStopping(patience=15, min_delta=1e-4)\n",
        "history    = {}\n",
        "start_epoch = 1\n",
        "\n",
        "\n",
        "if ckpt_path is not None:\n",
        "  name = configs[0]['name']\n",
        "  ckpt = torch.load(ckpt_path, map_location=device)\n",
        "  trainers[name].G.load_state_dict(ckpt['model'])\n",
        "  trainers[name].optG.load_state_dict(ckpt['opt'])\n",
        "  trainers[name].scaler.load_state_dict(ckpt['scaler'])\n",
        "  if ckpt.get('sched'):\n",
        "      trainers[name].lr_sched.load_state_dict(ckpt['sched'])\n",
        "  print(\"Loaded configuration from\", ckpt_path)\n",
        "\n",
        "  early_stop.best = ckpt.get('early_best', float('inf'))\n",
        "  trainers[name]._best_score = early_stop.best\n",
        "  start_epoch = ckpt.get('epoch', 0) + 1\n",
        "\n",
        "  history = ckpt.get('history', {})\n",
        "\n",
        "last_round_epoch = 1\n",
        "# TRAINING LOOP\n",
        "for epoch in range(start_epoch, num_epochs+1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # update loss weights\n",
        "    for cfg in configs:\n",
        "        loss_sched.apply(epoch, cfg)\n",
        "\n",
        "    # ---- TRAIN ----\n",
        "    epoch_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name    = cfg['name']\n",
        "        metrics = trainers[name].train_epoch(loader)\n",
        "        epoch_metrics[name] = metrics\n",
        "\n",
        "        # history (train)\n",
        "        hist = history.setdefault(name, {})\n",
        "        for k, v in metrics.items():\n",
        "            hist.setdefault(k, []).append(v)\n",
        "\n",
        "    # print train (now with MS-SSIM)\n",
        "    headers = [\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\"]\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']; m = epoch_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['loss_pixel']:.4e}\",\n",
        "            f\"{m['loss_perc']:.4e}\"    if cfg.get('use_perc', False)  else \"-\",\n",
        "            f\"{m['loss_attn']:.4e}\"    if cfg.get('use_attn', False)  else \"-\",\n",
        "            f\"{m['loss_lpips']:.4e}\"   if cfg.get('use_lpips', False) else \"-\",\n",
        "            f\"{m['loss_combined']:.4e}\",\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['msssim_overall']:.4f}\",\n",
        "            f\"{m['lpips_overall']:.4f}\",\n",
        "        ])\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "    # ---- VALIDATION ----\n",
        "    val_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']\n",
        "        vm = trainers[name].evaluate(val_loader)\n",
        "        val_metrics[name] = vm\n",
        "\n",
        "        # history (val)\n",
        "        hist = history[name]\n",
        "        for k, v in vm.items():\n",
        "            hist.setdefault('val_' + k, []).append(v)\n",
        "\n",
        "    ref_name = configs[0]['name']\n",
        "    ref_val  = val_metrics[ref_name]\n",
        "    score    = val_objective(ref_val)\n",
        "\n",
        "    # schedulers + early stop\n",
        "    for cfg in configs:\n",
        "        trainers[cfg['name']].lr_sched.step(val_objective(val_metrics[cfg['name']]))\n",
        "    early_stop.step(score)\n",
        "\n",
        "    # print val (now with MS-SSIM)\n",
        "    headers = [\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\",\"val_obj\"]\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']; m = val_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['loss_pixel']:.4e}\",\n",
        "            f\"{m['loss_perc']:.4e}\"    if cfg.get('use_perc', False)  else \"-\",\n",
        "            f\"{m['loss_attn']:.4e}\"    if cfg.get('use_attn', False)  else \"-\",\n",
        "            f\"{m['loss_lpips']:.4e}\"   if cfg.get('use_lpips', False) else \"-\",\n",
        "            f\"{m['loss_combined']:.4e}\",\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['msssim_overall']:.4f}\",\n",
        "            f\"{m['lpips_overall']:.4f}\",\n",
        "            f\"{val_objective(m):.4f}\",\n",
        "        ])\n",
        "    print(\"\\n\")\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "\n",
        "    # quick viz\n",
        "    if epoch % 5 ==0:\n",
        "        val_iter = iter(val_loader)\n",
        "        _, _, hr_val_b, lr_val_b, _ = next(val_iter)\n",
        "        lr_val = lr_val_b[:5].to(device)\n",
        "        hr_val = hr_val_b[:5].cpu()\n",
        "\n",
        "        lr_up_val = to01(F.interpolate(lr_val, size=(128,128), mode='bilinear', align_corners=False)).cpu()\n",
        "        recon_val = {}\n",
        "        with torch.no_grad():\n",
        "            for cfg in configs:\n",
        "                name = cfg['name']\n",
        "                sr = trainers[name].G.eval()(lr_val)\n",
        "                recon_val[name] = to01(sr).cpu()\n",
        "\n",
        "        fig, axes = plt.subplots(5, 2 + len(configs), figsize=(4*(2+len(configs)), 20))\n",
        "        for i in range(5):\n",
        "            row = axes[i]\n",
        "            row[0].imshow(lr_up_val[i].permute(1,2,0)); row[0].set_title(\"LR ↑\"); row[0].axis('off')\n",
        "            row[1].imshow(to01(hr_val[i]).permute(1,2,0)); row[1].set_title(\"HR GT\"); row[1].axis('off')\n",
        "            for j, cfg in enumerate(configs, start=2):\n",
        "                nm = cfg['name']\n",
        "                row[j].imshow(recon_val[nm][i].permute(1,2,0)); row[j].set_title(nm); row[j].axis('off')\n",
        "        plt.suptitle(f\"Epoch {epoch} — Validation Reconstructions\", fontsize=16)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "    # save best + periodic\n",
        "    best_dir = f\"./{ref_name}\"; os.makedirs(best_dir, exist_ok=True)\n",
        "    best_path = os.path.join(best_dir, f\"{ref_name}_best.pth\")\n",
        "    if score <= getattr(trainers[ref_name], \"_best_score\", float(\"inf\")):\n",
        "        torch.save(trainers[ref_name].G.state_dict(), best_path)\n",
        "        trainers[ref_name]._best_score = score\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        for cfg in configs:\n",
        "            name   = cfg['name']\n",
        "            last_round_epoch = epoch\n",
        "            folder = f\"./{name}\"; os.makedirs(folder, exist_ok=True)\n",
        "            path_to_save   =  f\"./{name}/{name}_epoch{epoch:03d}.pt\"\n",
        "            save_checkpoint(trainers[name], history, epoch, early_stop.best,path_to_save)\n",
        "\n",
        "    if early_stop.should_stop:\n",
        "        print(f\"\\nEarly stopping at epoch {epoch} (best val objective: {early_stop.best:.4f}).\")\n",
        "\n",
        "        # --- load the best checkpoint ---\n",
        "        best_ckpt = torch.load(best_path, map_location=device)\n",
        "        trainers[ref_name].G.load_state_dict(best_ckpt)\n",
        "\n",
        "        # --- recompute validation metrics for best model ---\n",
        "        best_metrics = trainers[ref_name].evaluate(val_loader)\n",
        "\n",
        "        print(\"\\n=== Best Model Validation Metrics ===\")\n",
        "        print(tabulate([[\n",
        "            ref_name,\n",
        "            f\"{best_metrics['loss_pixel']:.4e}\",\n",
        "            f\"{best_metrics['loss_perc']:.4e}\"    if configs[0].get('use_perc', False)  else \"-\",\n",
        "            f\"{best_metrics['loss_attn']:.4e}\"    if configs[0].get('use_attn', False)  else \"-\",\n",
        "            f\"{best_metrics['loss_lpips']:.4e}\"   if configs[0].get('use_lpips', False) else \"-\",\n",
        "            f\"{best_metrics['loss_combined']:.4e}\",\n",
        "            f\"{best_metrics['psnr_overall']:.2f}\",\n",
        "            f\"{best_metrics['ssim_overall']:.4f}\",\n",
        "            f\"{best_metrics['msssim_overall']:.4f}\",\n",
        "            f\"{best_metrics['lpips_overall']:.4f}\",\n",
        "            f\"{val_objective(best_metrics):.4f}\",\n",
        "        ]], headers=[\"Model\",\"pix\",\"perc\",\"attn\",\"lpips_loss\",\"comb\",\"PSNR\",\"SSIM\",\"MS-SSIM\",\"LPIPS\",\"val_obj\"], tablefmt=\"github\"))\n",
        "\n",
        "        # --- visualize best model ---\n",
        "        val_iter = iter(val_loader)\n",
        "        _, _, hr_val_b, lr_val_b, _ = next(val_iter)\n",
        "        lr_val = lr_val_b[:5].to(device)\n",
        "        hr_val = hr_val_b[:5].cpu()\n",
        "\n",
        "        lr_up_val = to01(F.interpolate(lr_val, size=(128,128), mode='bilinear', align_corners=False)).cpu()\n",
        "        recon_val = {}\n",
        "        with torch.no_grad():\n",
        "            for cfg in configs:\n",
        "                nm = cfg['name']\n",
        "                sr = trainers[nm].G.eval()(lr_val)\n",
        "                recon_val[nm] = to01(sr).cpu()\n",
        "\n",
        "        fig, axes = plt.subplots(5, 2 + len(configs), figsize=(4*(2+len(configs)), 20))\n",
        "        for i in range(5):\n",
        "            row = axes[i]\n",
        "            row[0].imshow(lr_up_val[i].permute(1,2,0)); row[0].set_title(\"LR ↑\"); row[0].axis('off')\n",
        "            row[1].imshow(to01(hr_val[i]).permute(1,2,0)); row[1].set_title(\"HR GT\"); row[1].axis('off')\n",
        "            for j, cfg in enumerate(configs, start=2):\n",
        "                nm = cfg['name']\n",
        "                row[j].imshow(recon_val[nm][i].permute(1,2,0)); row[j].set_title(f\"{nm} (best)\"); row[j].axis('off')\n",
        "        plt.suptitle(f\"Best Model — Validation Reconstructions\", fontsize=16)\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        # best metrics\n",
        "\n",
        "\n",
        "        break\n",
        "\n",
        "\n",
        "path_to_save   =  f\"./{name}/{name}_epoch{last_round_epoch:03d}.pt\"\n",
        "files.download(path_to_save)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0z5UtRlen4Ig"
      },
      "source": [
        "| Column          | Meaning                                                                                     | What It Tells You                                                                                                                                  |\n",
        "| --------------- | ------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Model**       | Name of the model/config used in training.                                                  | Helps identify which configuration the metrics refer to.                                                                                           |\n",
        "| **pix**         | Pixel-wise reconstruction loss (typically MSE) between SR output and ground truth HR image. | Lower is better; measures raw pixel accuracy but doesn’t account for perceptual quality.                                                           |\n",
        "| **perc**        | Perceptual loss (e.g., VGG-based feature difference).                                       | Lower means the SR image is closer in high-level visual features to the HR image. Large values are normal due to the scale of feature differences. |\n",
        "| **attn**        | Attention loss (if used).                                                                   | Quantifies how well the model attends to important regions (e.g., face parts). Not present (`-`) if attention is disabled.                         |\n",
        "| **lpips\\_loss** | Learned Perceptual Image Patch Similarity (LPIPS) metric.                                   | Lower is better; correlates well with human perception of image similarity.                                                                        |\n",
        "| **comb**        | Weighted sum of all active loss components.                                                 | This is the actual loss value used for optimization.                                                                                               |\n",
        "| **PSNR**        | Peak Signal-to-Noise Ratio (in dB).                                                         | Higher is better; measures reconstruction fidelity at the pixel level.                                                                             |\n",
        "| **SSIM**        | Structural Similarity Index.                                                                | Higher is better (max = 1); measures structural similarity between SR and HR images.                                                               |\n",
        "| **MS-SSIM**     | Multi-Scale SSIM.                                                                           | Higher is better; similar to SSIM but considers image structures at multiple scales.                                                               |\n",
        "| **LPIPS**       | LPIPS metric computed as evaluation (same scale as `lpips_loss`).                           | Lower means better perceptual similarity on the validation set.                                                                                    |\n",
        "| **val\\_obj**    | Validation objective score (from `val_objective`).                                          | Lower is better; combines LPIPS, SSIM, and PSNR into a single scalar for model selection/early stopping.                                           |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlz7gPA-QzYy"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# pick the model you want to plot (you have only one in configs now)\n",
        "name = list(history.keys())[0]\n",
        "H = history[name]\n",
        "\n",
        "def _ema(values, alpha=0.9):\n",
        "    \"\"\"Simple EMA over a 1D list/array. Returns a list of same length.\"\"\"\n",
        "    if values is None or len(values) == 0:\n",
        "        return []\n",
        "    out = []\n",
        "    m = None\n",
        "    for v in values:\n",
        "        v = float(v)\n",
        "        m = v if (m is None) else alpha * m + (1 - alpha) * v\n",
        "        out.append(m)\n",
        "    return out\n",
        "\n",
        "def plot_metric(metric_key, title, ylabel=\"\", invert=False, ema_alpha=0.9, show_raw=True):\n",
        "    \"\"\"\n",
        "    Plot train/val curves with optional EMA smoothing.\n",
        "    - ema_alpha: None to disable EMA; else e.g. 0.9\n",
        "    - show_raw: whether to also draw raw curves (faint) alongside EMA\n",
        "    \"\"\"\n",
        "    train = H.get(metric_key, [])\n",
        "    val   = H.get(f\"val_{metric_key}\", [])\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "\n",
        "    # Train\n",
        "    if train:\n",
        "        if ema_alpha is not None:\n",
        "            tr_ema = _ema(train, alpha=ema_alpha)\n",
        "            plt.plot(tr_ema, label=f\"train (EMA {ema_alpha})\")\n",
        "            if show_raw:\n",
        "                plt.plot(train, label=\"train (raw)\", linestyle=\"--\", alpha=0.35)\n",
        "        else:\n",
        "            plt.plot(train, label=\"train\")\n",
        "\n",
        "    # Val\n",
        "    if val:\n",
        "        if ema_alpha is not None:\n",
        "            va_ema = _ema(val, alpha=ema_alpha)\n",
        "            plt.plot(va_ema, label=f\"val (EMA {ema_alpha})\")\n",
        "            if show_raw:\n",
        "                plt.plot(val, label=\"val (raw)\", linestyle=\"--\", alpha=0.35)\n",
        "        else:\n",
        "            plt.plot(val, label=\"val\")\n",
        "\n",
        "    if invert:\n",
        "        plt.gca().invert_yaxis()\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(ylabel if ylabel else metric_key)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# === Losses ===\n",
        "plot_metric(\"loss_combined\", \"Combined Loss\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "plot_metric(\"loss_pixel\",    \"Pixel Loss (MSE)\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_perc\" in H:  plot_metric(\"loss_perc\",  \"Perceptual Loss\", ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_attn\" in H:  plot_metric(\"loss_attn\",  \"Attention Loss\",  ylabel=\"Loss\", ema_alpha=0.9)\n",
        "if \"loss_lpips\" in H: plot_metric(\"loss_lpips\", \"LPIPS Loss\",      ylabel=\"Loss\", ema_alpha=0.9)\n",
        "\n",
        "# === Image Quality Metrics ===\n",
        "plot_metric(\"psnr_overall\",   \"PSNR\",     ylabel=\"dB\",     ema_alpha=0.9)\n",
        "plot_metric(\"ssim_overall\",   \"SSIM\",     ylabel=\"Score\",  ema_alpha=0.9)\n",
        "plot_metric(\"msssim_overall\", \"MS-SSIM\",  ylabel=\"Score\",  ema_alpha=0.9)\n",
        "plot_metric(\"lpips_overall\",  \"LPIPS\",    ylabel=\"Distance\", invert=True, ema_alpha=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHnUO8XeugKj"
      },
      "outputs": [],
      "source": [
        "# This cleanup sequence is only used in case of runtime errors or memory issues.\n",
        "# It manually deletes large objects, triggers Python’s garbage collector,\n",
        "# clears PyTorch’s GPU memory cache, resets memory stats, and synchronizes the device\n",
        "# to fully release unused GPU resources before retrying or exiting.\n",
        "\n",
        "\n",
        "# 1) delete big objects\n",
        "#del trainers, history\n",
        "\n",
        "# 2) force Python GC\n",
        "#import gc\n",
        "#gc.collect()\n",
        "\n",
        "# 3) release PyTorch cache\n",
        "#import torch\n",
        "#torch.cuda.empty_cache()\n",
        "#torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# 4) (for good measure) synchronize\n",
        "#torch.cuda.synchronize()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}