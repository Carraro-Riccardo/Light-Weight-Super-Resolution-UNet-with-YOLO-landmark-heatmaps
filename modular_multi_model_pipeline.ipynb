{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq ultralytics\n",
        "!pip install -qq torchmetrics"
      ],
      "metadata": {
        "id": "hVilGaX2n02x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import torch\n",
        "path = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "#dowload heatmaps\n",
        "!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps_10k.h5\n",
        "\n",
        "#uncomment the following line to use the 50k version of the dataset\n",
        "#!curl -L -o heatmaps.h5 https://huggingface.co/datasets/RiccardoCarraro/heatmaps/resolve/main/heatmaps.h5"
      ],
      "metadata": {
        "id": "hhQkrGmuneWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import h5py\n",
        "import torch\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.transforms import InterpolationMode\n",
        "from os.path import join, splitext\n",
        "from PIL import Image\n",
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "class CelebDataSet(Dataset):\n",
        "    \"\"\"\n",
        "    CelebA dataset with optional landmark-heatmap loading from HDF5.\n",
        "\n",
        "    Returns: (x2, x4, hr, lr, heatmap)\n",
        "      - x2: 32Ã—32 target tensor\n",
        "      - x4: 64Ã—64 target tensor\n",
        "      - hr: 128Ã—128 target tensor\n",
        "      - lr: 16Ã—16 input tensor\n",
        "      - heatmap: 1Ã—128Ã—128 float tensor\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data_path: str = './dataset/',\n",
        "        state: str = 'train',\n",
        "        data_augmentation: bool = False,\n",
        "        heatmap_h5: str = None,\n",
        "    ):\n",
        "        self.main_path = data_path\n",
        "        self.state = state\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.img_path = join(self.main_path, 'img_align_celeba/img_align_celeba/')\n",
        "        self.eval_partition_path = join(self.main_path, 'list_eval_partition.csv')\n",
        "\n",
        "        # load train/val/test split\n",
        "        train_list, val_list, test_list = [], [], []\n",
        "        with open(self.eval_partition_path, 'r') as f:\n",
        "            reader = csv.reader(f)\n",
        "            for fname, split in reader:\n",
        "                fname, split = fname.strip(), split.strip()\n",
        "                if split == '0':\n",
        "                    train_list.append(fname)\n",
        "                elif split == '1':\n",
        "                    val_list.append(fname)\n",
        "                else:\n",
        "                    test_list.append(fname)\n",
        "\n",
        "        if state == 'train':\n",
        "            self.image_list = sorted(train_list)\n",
        "        elif state == 'val':\n",
        "            self.image_list = sorted(val_list)\n",
        "        else:\n",
        "            self.image_list = sorted(test_list)\n",
        "\n",
        "        # transforms\n",
        "        if state=='train' and data_augmentation:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "                transforms.RandomRotation(\n",
        "                    20,\n",
        "                    interpolation=InterpolationMode.BILINEAR\n",
        "                ),\n",
        "                transforms.ColorJitter(0.4,0.4,0.4,0.1)\n",
        "            ])\n",
        "        else:\n",
        "            self.pre_process = transforms.Compose([\n",
        "                transforms.CenterCrop((178,178)),\n",
        "                transforms.Resize((128,128)),\n",
        "            ])\n",
        "\n",
        "        self.totensor = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),\n",
        "        ])\n",
        "        self.down64 = transforms.Resize((64,64))\n",
        "        self.down32 = transforms.Resize((32,32))\n",
        "        self.down16 = transforms.Resize((16,16))\n",
        "\n",
        "        # ACTUALLY load heatmaps into memory\n",
        "        if heatmap_h5:\n",
        "            with h5py.File(heatmap_h5, 'r') as h5_file:\n",
        "                # Load the entire heatmap dataset into RAM\n",
        "                self.heatmaps = np.array(h5_file['heatmaps'])  # Shape: (N, 128, 128)\n",
        "        else:\n",
        "            self.heatmaps = None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # load image\n",
        "        fname = self.image_list[index]\n",
        "        img = Image.open(join(self.img_path, fname)).convert('RGB')\n",
        "        img = self.pre_process(img)\n",
        "\n",
        "        # build multi-scale\n",
        "        x4 = self.down64(img)    # 64x64\n",
        "        x2 = self.down32(x4)     # 32x32\n",
        "        lr = self.down16(x2)     # 16x16\n",
        "\n",
        "        # to tensor\n",
        "        hr_tensor = self.totensor(img)\n",
        "        x4_tensor = self.totensor(x4)\n",
        "        x2_tensor = self.totensor(x2)\n",
        "        lr_tensor = self.totensor(lr)\n",
        "\n",
        "        # load heatmap (already 128Ã—128) - NOW FROM MEMORY!\n",
        "        if self.heatmaps is not None:\n",
        "            hm = self.heatmaps[index]              # numpy array (128,128) FROM MEMORY\n",
        "            heat = torch.from_numpy(hm.copy()).unsqueeze(0)  # (1,128,128)\n",
        "        else:\n",
        "            heat = torch.zeros(1,128,128)\n",
        "\n",
        "        return x2_tensor, x4_tensor, hr_tensor, lr_tensor, heat"
      ],
      "metadata": {
        "id": "HU9cNqQklC0y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"Efficient DoubleConv with LeakyReLU activation.\"\"\"\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upsampling followed by DoubleConv, with skip connection.\"\"\"\n",
        "    def __init__(self, in_ch, skip_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.conv = DoubleConv(in_ch + skip_ch, out_ch)\n",
        "    def forward(self, x, skip):\n",
        "        x = self.up(x)\n",
        "        x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "class UpLearn(nn.Module):\n",
        "    \"\"\"Simpler learned upsampling block.\"\"\"\n",
        "    def __init__(self, ch):\n",
        "        super().__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.ConvTranspose2d(ch, ch, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.up(x)\n",
        "\n",
        "class SuperResolutionUNet(nn.Module):\n",
        "    \"\"\"Efficient U-Net super-resolution (16x16 â†’ 128x128).\"\"\"\n",
        "    def __init__(self, in_channels=3, base_filters=32, out_channels=3):\n",
        "        super().__init__()\n",
        "        # Encoder (depth reduced)\n",
        "        self.enc1 = DoubleConv(in_channels, base_filters)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.enc2 = DoubleConv(base_filters, base_filters*2)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.enc3 = DoubleConv(base_filters*2, base_filters*4)\n",
        "        self.pool3 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Bottleneck (lighter)\n",
        "        self.bottleneck = DoubleConv(base_filters*4, base_filters*8)\n",
        "\n",
        "        # Decoder\n",
        "        self.up3 = UpBlock(base_filters*8, base_filters*4, base_filters*4)\n",
        "        self.up2 = UpBlock(base_filters*4, base_filters*2, base_filters*2)\n",
        "        self.up1 = UpBlock(base_filters*2, base_filters, base_filters)\n",
        "\n",
        "        # Learned upsampling (simplified, 16â†’32â†’64â†’128)\n",
        "        self.up_learn1 = UpLearn(base_filters)\n",
        "        self.up_learn2 = UpLearn(base_filters)\n",
        "        self.up_learn3 = UpLearn(base_filters)\n",
        "\n",
        "        # Final projection\n",
        "        self.final_conv = nn.Conv2d(base_filters, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        e1 = self.enc1(x); p1 = self.pool1(e1)\n",
        "        e2 = self.enc2(p1); p2 = self.pool2(e2)\n",
        "        e3 = self.enc3(p2); p3 = self.pool3(e3)\n",
        "\n",
        "        b = self.bottleneck(p3)\n",
        "\n",
        "        d3 = self.up3(b, e3)\n",
        "        d2 = self.up2(d3, e2)\n",
        "        d1 = self.up1(d2, e1)\n",
        "\n",
        "        u1 = self.up_learn1(d1)  # 32x32\n",
        "        u2 = self.up_learn2(u1)  # 64x64\n",
        "        u3 = self.up_learn3(u2)  # 128x128\n",
        "\n",
        "        out = self.final_conv(u3)\n",
        "        up_input = F.interpolate(x, size=out.shape[2:], mode='bilinear', align_corners=True)\n",
        "        return out + up_input"
      ],
      "metadata": {
        "id": "C0i-twj4vWzD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2: loss definitions\n",
        "import h5py\n",
        "\n",
        "# pixel\n",
        "pixel_crit = nn.MSELoss()\n",
        "\n",
        "# perceptual\n",
        "from torchvision.models import vgg16\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models import vgg16\n",
        "\n",
        "class VGGPerceptualLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        vgg = vgg16(pretrained=True).features.eval()\n",
        "        # qui Ã¨ cruciale iterare sui parametri, non sui moduli\n",
        "        for p in vgg.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        # estraiamo i tre slice\n",
        "        self.slice1 = nn.Sequential(*vgg[:4])   # fino a conv1_2\n",
        "        self.slice2 = nn.Sequential(*vgg[4:9])  # fino a conv2_2\n",
        "        self.slice3 = nn.Sequential(*vgg[9:16]) # fino a conv3_3\n",
        "\n",
        "        # buffers per la normalizzazione\n",
        "        self.register_buffer('mean', torch.tensor([0.485,0.456,0.406]).view(1,3,1,1))\n",
        "        self.register_buffer('std',  torch.tensor([0.229,0.224,0.225]).view(1,3,1,1))\n",
        "\n",
        "    def normalize(self, x):\n",
        "        # da [-1,1] a [0,1] poi standard VGG\n",
        "        x = (x + 1) / 2\n",
        "        return (x - self.mean) / self.std\n",
        "\n",
        "    def forward(self, sr, hr):\n",
        "        sr_n, hr_n = self.normalize(sr), self.normalize(hr)\n",
        "        f1_sr, f1_hr = self.slice1(sr_n), self.slice1(hr_n)\n",
        "        f2_sr, f2_hr = self.slice2(f1_sr), self.slice2(f1_hr)\n",
        "        f3_sr, f3_hr = self.slice3(f2_sr), self.slice3(f2_hr)\n",
        "        return (\n",
        "            F.mse_loss(f1_sr, f1_hr)\n",
        "          + F.mse_loss(f2_sr, f2_hr)\n",
        "          + F.mse_loss(f3_sr, f3_hr)\n",
        "        )\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "perceptual_crit = VGGPerceptualLoss().to(device)\n",
        "\n",
        "# attention\n",
        "def attention_loss(sr, hr, heat, eps=1e-6):\n",
        "    hmin = heat.flatten(1).min(1)[0].view(-1,1,1,1)\n",
        "    hmax = heat.flatten(1).max(1)[0].view(-1,1,1,1)\n",
        "    hn = (heat - hmin)/(hmax-hmin+eps)\n",
        "    return (hn.expand_as(sr)*(sr-hr).abs()).mean()\n",
        "\n",
        "# GAN hinge\n",
        "def d_hinge(real, fake):\n",
        "    return F.relu(1-real).mean()+F.relu(1+fake).mean()\n",
        "def g_hinge(fake):\n",
        "    return -fake.mean()"
      ],
      "metadata": {
        "id": "k4zij_PllY40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import torchvision.transforms as transforms\n",
        "from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "import numpy as np\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, cfg):\n",
        "        self.cfg = cfg\n",
        "        self.G   = SuperResolutionUNet().to(device)\n",
        "        self.optG = torch.optim.Adam(self.G.parameters(), lr=cfg['lr_g'])\n",
        "        self.scaler = torch.amp.GradScaler(device)\n",
        "\n",
        "    def compute_heatmap_focused_metrics(self, sr, hr, heat):\n",
        "        \"\"\"\n",
        "        Compute metrics that specifically measure how well the model\n",
        "        performs in regions highlighted by heatmaps.\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            batch_size = sr.shape[0]\n",
        "            metrics = {\n",
        "                'psnr_overall': 0.0,\n",
        "                'ssim_overall': 0.0,\n",
        "                'psnr_focused': 0.0,  # PSNR in high-attention regions\n",
        "                'ssim_focused': 0.0,  # SSIM in high-attention regions\n",
        "                'mse_focused': 0.0,   # MSE in high-attention regions\n",
        "            }\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                sr_i = sr[i:i+1]\n",
        "                hr_i = hr[i:i+1]\n",
        "                heat_i = heat[i] if heat is not None else None\n",
        "\n",
        "                # Overall metrics\n",
        "                mse_overall = F.mse_loss(sr_i, hr_i)\n",
        "                psnr_overall = 10 * torch.log10(1.0 / (mse_overall + 1e-8))\n",
        "                ssim_overall = ssim(sr_i, hr_i, data_range=1.0)\n",
        "\n",
        "                metrics['psnr_overall'] += psnr_overall.item()\n",
        "                metrics['ssim_overall'] += ssim_overall.item()\n",
        "\n",
        "                # Focused metrics (only if heatmaps available)\n",
        "                if heat_i is not None:\n",
        "                    heat_i = heat_i.float()\n",
        "                    # Create binary mask for top 30% of heatmap values\n",
        "                    heat_flat = heat_i.flatten()\n",
        "                    threshold = torch.quantile(heat_flat, 0.7)  # Top 30%\n",
        "                    focus_mask = (heat_i >= threshold).float()\n",
        "\n",
        "                    # Expand mask to match image channels\n",
        "                    focus_mask_3ch = focus_mask.expand_as(sr_i)\n",
        "\n",
        "                    # Compute metrics only in focused regions\n",
        "                    sr_focused = sr_i * focus_mask_3ch\n",
        "                    hr_focused = hr_i * focus_mask_3ch\n",
        "\n",
        "                    # MSE in focused regions\n",
        "                    mse_focused = F.mse_loss(sr_focused, hr_focused)\n",
        "                    psnr_focused = 10 * torch.log10(1.0 / (mse_focused + 1e-8))\n",
        "\n",
        "                    # For SSIM, we need to be careful with masked regions\n",
        "                    # Use a smaller window and only compute where mask is active\n",
        "                    focus_area = focus_mask.sum()\n",
        "                    if focus_area > 100:  # Only if we have enough pixels\n",
        "                        ssim_focused = ssim(sr_focused, hr_focused, data_range=1.0)\n",
        "                        metrics['ssim_focused'] += ssim_focused.item()\n",
        "\n",
        "                    metrics['psnr_focused'] += psnr_focused.item()\n",
        "                    metrics['mse_focused'] += mse_focused.item()\n",
        "\n",
        "            # Average over batch\n",
        "            for key in metrics:\n",
        "                metrics[key] /= batch_size\n",
        "\n",
        "            return metrics\n",
        "\n",
        "    def train_epoch(self, loader):\n",
        "        agg = {\n",
        "            # Loss components (separate for analysis)\n",
        "            'loss_pixel': 0, 'loss_perc': 0, 'loss_attn': 0, 'loss_combined': 0,\n",
        "            # Overall quality metrics\n",
        "            'psnr_overall': 0, 'ssim_overall': 0,\n",
        "            # Heatmap-focused metrics (the key ones for your research!)\n",
        "            'psnr_focused': 0, 'ssim_focused': 0, 'mse_focused': 0\n",
        "        }\n",
        "        n = 0\n",
        "        self.G.train()\n",
        "\n",
        "        for _, _, hr, lr, heat in tqdm(loader, desc=f\"Training {self.cfg['name']}\"):\n",
        "            lr = lr.to(device)\n",
        "            hr = hr.to(device)\n",
        "            heat = heat.to(device) if self.cfg['use_attn'] else None\n",
        "\n",
        "            self.optG.zero_grad()\n",
        "\n",
        "            with torch.amp.autocast(device_type='cuda'):\n",
        "                sr = self.G(lr)\n",
        "                Lpix  = pixel_crit(sr, hr)\n",
        "                Lperc = perceptual_crit(sr, hr) if self.cfg['use_perc'] else 0\n",
        "                Lattn = attention_loss(sr, hr, heat) if self.cfg['use_attn'] else 0\n",
        "                loss  = (Lpix + self.cfg['w_perc']*Lperc + self.cfg['w_attn']*Lattn)\n",
        "\n",
        "            self.scaler.scale(loss).backward()\n",
        "            self.scaler.step(self.optG)\n",
        "            self.scaler.update()\n",
        "\n",
        "            # Accumulate loss components separately\n",
        "            agg['loss_pixel'] += Lpix.item()\n",
        "            agg['loss_perc'] += Lperc.item() if self.cfg['use_perc'] else 0\n",
        "            agg['loss_attn'] += Lattn.item() if self.cfg['use_attn'] else 0\n",
        "            agg['loss_combined'] += loss.item()\n",
        "\n",
        "            # Compute quality metrics every few batches to avoid slowdown\n",
        "            if n % 5 == 0:  # Every 5th batch\n",
        "                metrics = self.compute_heatmap_focused_metrics(sr.detach(), hr, heat)\n",
        "                for key, value in metrics.items():\n",
        "                    agg[key] += value\n",
        "\n",
        "            n += 1\n",
        "\n",
        "        # Average all metrics\n",
        "        result = {}\n",
        "        for key, value in agg.items():\n",
        "            if 'focused' in key or 'overall' in key:\n",
        "                # These were computed every 5th batch\n",
        "                result[key] = value / (n // 5 + 1)\n",
        "            else:\n",
        "                # Loss components computed every batch\n",
        "                result[key] = value / n\n",
        "\n",
        "        return result\n",
        "\n",
        "    def evaluate(self, loader, num_samples=500):\n",
        "        \"\"\"\n",
        "        Comprehensive evaluation with focus on heatmap-guided performance\n",
        "        \"\"\"\n",
        "        self.G.eval()\n",
        "        agg = {\n",
        "            'psnr_overall': 0, 'ssim_overall': 0,\n",
        "            'psnr_focused': 0, 'ssim_focused': 0, 'mse_focused': 0,\n",
        "            'focus_improvement': 0  # How much better is focused vs overall\n",
        "        }\n",
        "        n = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _, _, hr, lr, heat in tqdm(loader, desc=f\"Evaluating {self.cfg['name']}\"):\n",
        "                if n >= num_samples:\n",
        "                    break\n",
        "\n",
        "                lr = lr.to(device)\n",
        "                hr = hr.to(device)\n",
        "                heat = heat.to(device) if heat is not None else None\n",
        "\n",
        "                sr = self.G(lr)\n",
        "                metrics = self.compute_heatmap_focused_metrics(sr, hr, heat)\n",
        "\n",
        "                for key, value in metrics.items():\n",
        "                    agg[key] += value\n",
        "\n",
        "                # Compute improvement ratio (key metric for your research!)\n",
        "                if heat is not None and metrics['psnr_focused'] > 0:\n",
        "                    improvement = metrics['psnr_focused'] / (metrics['psnr_overall'] + 1e-8)\n",
        "                    agg['focus_improvement'] += improvement\n",
        "\n",
        "                n += 1\n",
        "\n",
        "        return {k: v/n for k, v in agg.items()}\n",
        "\n",
        "\n",
        "configs = [\n",
        "    dict(name='mse+perceptual',     use_attn=False, use_perc=True,  w_attn=0.0,  w_perc=0.01,    lr_g=1e-4),\n",
        "    dict(name='mse+perc+heatmap',   use_attn=True,  use_perc=True,  w_attn=1.5,  w_perc=0.01,    lr_g=1e-4),\n",
        "]\n"
      ],
      "metadata": {
        "id": "U0-5vDrLhyKh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "data_path  = path\n",
        "heat_h5    = './heatmaps.h5'\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "\n",
        "NUM_SAMPLES = 1000\n",
        "with h5py.File('heatmaps.h5', 'r') as f:\n",
        "    heatmaps = f['heatmaps']\n",
        "    NUM_SAMPLES = heatmaps.shape[0]\n",
        "    print(\"Number of heatmaps:\", NUM_SAMPLES)\n",
        "\n",
        "# â€”â€”â€” Data loader & fixed sample â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "full_ds  = CelebDataSet(data_path, 'train', heatmap_h5=heat_h5)\n",
        "small_ds = Subset(full_ds, range(NUM_SAMPLES))\n",
        "loader   = DataLoader(small_ds, batch_size=batch_size, shuffle=True,\n",
        "                      num_workers=2, pin_memory=True)\n",
        "\n",
        "val_ds = CelebDataSet(data_path, 'val', heatmap_h5=heat_h5)\n",
        "small_val_ds = Subset(val_ds, range(int(0.2 * NUM_SAMPLES)))\n",
        "val_loader = DataLoader(small_val_ds, batch_size=batch_size, shuffle=False,\n",
        "                        num_workers=2, pin_memory=True)\n",
        "\n",
        "print(\"Train samples: \", len(loader.dataset))\n",
        "print(\"Val samples:   \", len(val_loader.dataset))\n",
        "\n",
        "# grab one batch so we have a fixed sample for visualization\n",
        "data_iter    = iter(loader)\n",
        "_, _, hr_f, lr_f, heat_f = next(data_iter)\n",
        "lr_vis, hr_vis, heat_vis = [\n",
        "    t.to(device, non_blocking=True)\n",
        "    for t in (lr_f[0:1], hr_f[0:1], heat_f[0:1])\n",
        "]\n",
        "\n",
        "# â€”â€”â€” Training loop â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "# initialize trainers & empty history\n",
        "trainers = {cfg['name']: Trainer(cfg) for cfg in configs}\n",
        "history  = {}  # we'll build this on the fly\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    print(f\"\\n=== Epoch {epoch}/{num_epochs} ===\")\n",
        "\n",
        "    # 1) Train each model and gather metrics\n",
        "    epoch_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name    = cfg['name']\n",
        "        metrics = trainers[name].train_epoch(loader)\n",
        "        epoch_metrics[name] = metrics\n",
        "\n",
        "        # record history (creates list if needed)\n",
        "        hist = history.setdefault(name, {})\n",
        "        for k, v in metrics.items():\n",
        "            hist.setdefault(k, []).append(v)\n",
        "\n",
        "    # 2) Print all metrics in a table\n",
        "    headers = [\n",
        "        \"Model\",\n",
        "        \"pix\",   \"perc\",  \"attn\", \"comb\",\n",
        "        \"PSNR_o\",\"SSIM_o\",\n",
        "        \"PSNR_f\",\"SSIM_f\",\"MSE_f\"\n",
        "    ]\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']\n",
        "        m    = epoch_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['loss_pixel']:.4e}\",\n",
        "            f\"{m['loss_perc']:.4e}\"    if cfg['use_perc'] else \"-\",\n",
        "            f\"{m['loss_attn']:.4e}\"    if cfg['use_attn'] else \"-\",\n",
        "            f\"{m['loss_combined']:.4e}\",\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['psnr_focused']:.2f}\",\n",
        "            f\"{m['ssim_focused']:.4f}\",\n",
        "            f\"{m['mse_focused']:.4e}\"\n",
        "        ])\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "    # â€”â€”â€” VALIDATION METRICS â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "    val_metrics = {}\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']\n",
        "        val_metrics[name] = trainers[name].evaluate(val_loader)\n",
        "        # record val history\n",
        "        hist = history[name]\n",
        "        for k, v in val_metrics[name].items():\n",
        "            hist.setdefault('val_' + k, []).append(v)\n",
        "\n",
        "    table = []\n",
        "    for cfg in configs:\n",
        "        name = cfg['name']\n",
        "        m    = val_metrics[name]\n",
        "        table.append([\n",
        "            name,\n",
        "            f\"{m['psnr_overall']:.2f}\",\n",
        "            f\"{m['ssim_overall']:.4f}\",\n",
        "            f\"{m['psnr_focused']:.2f}\",\n",
        "            f\"{m['ssim_focused']:.4f}\",\n",
        "            f\"{m['mse_focused']:.4e}\",\n",
        "            f\"{m['focus_improvement']:.2f}\"\n",
        "        ])\n",
        "    print(tabulate(table, headers=headers, tablefmt=\"github\"))\n",
        "\n",
        "    # â€”â€”â€” VALIDATION VISUALIZATION â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
        "    # grab first batch from val_loader\n",
        "    val_iter = iter(val_loader)\n",
        "    _, _, hr_val_b, lr_val_b, heat_val_b = next(val_iter)\n",
        "    lr_val = lr_val_b[:5].to(device)\n",
        "    hr_val = hr_val_b[:5].cpu()\n",
        "\n",
        "    lr_up_val = F.interpolate(lr_val, size=(128,128), mode='bilinear', align_corners=False).cpu()\n",
        "\n",
        "    # get reconstructions on val\n",
        "    recon_val = {cfg['name']: [] for cfg in configs}\n",
        "    with torch.no_grad():\n",
        "        for cfg in configs:\n",
        "            name = cfg['name']\n",
        "            G = trainers[name].G.eval()\n",
        "            sr = G(lr_val)\n",
        "            recon_val[name] = sr.cpu()\n",
        "\n",
        "    # plot same as before but for val\n",
        "    fig, axes = plt.subplots(5, 2 + len(configs), figsize=(4*(2+len(configs)), 20))\n",
        "    for i in range(5):\n",
        "        row = axes[i]\n",
        "        # LR upsampled\n",
        "        row[0].imshow(lr_up_val[i].permute(1,2,0).clamp(0,1))\n",
        "        row[0].set_title(\"LR â†‘\")\n",
        "        row[0].axis('off')\n",
        "        # HR GT\n",
        "        row[1].imshow(hr_val[i].permute(1,2,0).clamp(0,1))\n",
        "        row[1].set_title(\"HR GT\")\n",
        "        row[1].axis('off')\n",
        "        # SR outputs\n",
        "        for j, cfg in enumerate(configs, start=2):\n",
        "            nm = cfg['name']\n",
        "            row[j].imshow(recon_val[nm][i].permute(1,2,0).clamp(0,1))\n",
        "            row[j].set_title(nm)\n",
        "            row[j].axis('off')\n",
        "    plt.suptitle(f\"Epoch {epoch} â€” Validation Reconstructions\", fontsize=16)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # 4) Save checkpoints every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        for cfg in configs:\n",
        "            name   = cfg['name']\n",
        "            folder = f\"./{name}\"\n",
        "            os.makedirs(folder, exist_ok=True)\n",
        "            path   = os.path.join(folder, f\"{name}_epoch{epoch:03d}.pth\")\n",
        "            torch.save(trainers[name].G.state_dict(), path)\n"
      ],
      "metadata": {
        "id": "fR8am-MfnEYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ“Š Interpretazione delle metriche di training\n",
        "\n",
        "Questa tabella riporta diverse metriche calcolate per ogni modello durante il training:\n",
        "\n",
        "| Colonna       | Significato |\n",
        "|---------------|-------------|\n",
        "| **Model**     | Nome del modello (configurazione usata) |\n",
        "| **pix**       | Pixel-wise loss (MSE): differenza media pixel a pixel rispetto al ground truth |\n",
        "| **perc**      | Perceptual loss (VGG): distanza tra le immagini nello spazio delle feature |\n",
        "| **attn**      | Attention loss: penalizza gli errori nelle regioni salienti definite dalla heatmap |\n",
        "| **comb**      | Loss totale: somma pesata di tutte le componenti di loss (`pix + w_perc * perc + w_attn * attn`) |\n",
        "| **PSNR_o**    | PSNR globale: misura la fedeltÃ  dell'immagine generata rispetto al ground truth su tutta l'immagine |\n",
        "| **SSIM_o**    | SSIM globale: misura la somiglianza strutturale tra output e ground truth |\n",
        "| **PSNR_f**    | PSNR focalizzato: PSNR calcolato **solo nelle regioni salienti** (top 30% della heatmap) |\n",
        "| **SSIM_f**    | SSIM focalizzato: SSIM calcolato **solo nelle regioni salienti** |\n",
        "| **MSE_f**     | MSE focalizzato: errore quadratico medio **solo nelle regioni salienti** |\n",
        "\n",
        "### â„¹ï¸ Note utili:\n",
        "- Se un modello **non usa heatmap**, le metriche focalizzate (`PSNR_f`, `SSIM_f`, `MSE_f`) saranno pari a 0.\n",
        "- Se `PSNR_f` Ã¨ **molto piÃ¹ alto di `PSNR_o`**, significa che l'attenzione sta aiutando il modello a ricostruire meglio le **zone piÃ¹ importanti** (es. occhi, bocca).\n",
        "- La `perceptual loss` puÃ² migliorare il realismo visivo, ma spesso **abbassa** il PSNR/MSE perchÃ© non forza una corrispondenza pixel a pixel.\n",
        "- `comb` ti dÃ  un'idea di quanto â€œpesaâ€ ogni loss nel training: un valore alto puÃ² indicare un bilanciamento non ottimale."
      ],
      "metadata": {
        "id": "0z5UtRlen4Ig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) delete big objects\n",
        "del trainers, history\n",
        "\n",
        "# 2) force Python GC\n",
        "import gc\n",
        "gc.collect()\n",
        "\n",
        "# 3) release PyTorch cache\n",
        "import torch\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "# 4) (for good measure) synchronize\n",
        "torch.cuda.synchronize()\n"
      ],
      "metadata": {
        "id": "EHnUO8XeugKj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}